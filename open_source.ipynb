{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V4_aE-OENhKR"},"outputs":[],"source":["import time\n","\n","import numpy as np\n","\n","from sentence_transformers import SentenceTransformer\n","from transformers import pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8jwCwPheNhKU"},"outputs":[],"source":["model = SentenceTransformer('ggrn/e5-small-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wexLuiphNhKU"},"outputs":[],"source":["model = SentenceTransformer('BAAI/bge-small-en-v1.5')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cz1kMeGNhKU"},"outputs":[],"source":["with open('textbook.txt', 'r') as f:\n","    textbook = f.read()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZLuYLTlJNhKU","outputId":"fcd1bc5b-e96e-410e-aef3-cde3ebff8dd8"},"outputs":[{"name":"stdout","output_type":"stream","text":["19\n"]}],"source":["chunk_size = 2048\n","\n","chunks = [textbook[i:i+chunk_size] for i in range(0, len(textbook), chunk_size)]\n","\n","print(len(chunks))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VeIsr-MANhKU","outputId":"83639daa-6452-47fd-d7d9-924d4a357ec9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken to vectorize using E5-small-V2: 3.31s\n"]}],"source":["def get_text_embedding(input):\n","    embeddings_batch_response = model.encode(input, normalize_embeddings=True)\n","    return embeddings_batch_response\n","\n","start = time.time()\n","text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n","end = time.time()\n","\n","print(f\"Time taken to vectorize using E5-small-V2: {round(end-start, 2)}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vU0I1VidNhKU","outputId":"b677d191-18ce-4ef8-8387-a8e1bf9caba1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Time taken to vectorize using BGE-small: 3.57s\n"]}],"source":["def get_text_embedding(input):\n","    embeddings_batch_response = model.encode(input, normalize_embeddings=True)\n","    return embeddings_batch_response\n","\n","start = time.time()\n","text_embeddings = np.array([get_text_embedding(chunk) for chunk in chunks])\n","end = time.time()\n","\n","print(f\"Time taken to vectorize using BGE-small: {round(end-start, 2)}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vr-V_TWnNhKV"},"outputs":[],"source":["import faiss\n","\n","d = text_embeddings.shape[1]\n","index = faiss.IndexFlatL2(d)\n","index.add(text_embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7FnILrPoNhKV"},"outputs":[],"source":["questions = []\n","\n","with open(\"questions.txt\") as f:\n","    for line in f:\n","        if line.strip():\n","            questions.append(line.strip())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9huXlETONhKV"},"outputs":[],"source":["chatAgent = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zG3foKG5NhKV","outputId":"c71b2ff7-f7da-41b4-8049-c36f004cefe0"},"outputs":[{"name":"stdout","output_type":"stream","text":["Answering question: 1\n","Answering question: 2\n","Answering question: 3\n","Answering question: 4\n","Answering question: 5\n","Answering question: 6\n","Answering question: 7\n","Answering question: 8\n","Time taken to answer questions on E5-small-V2: 964.73s\n"]}],"source":["predicted_answers = []\n","\n","start = time.time()\n","for i, question in enumerate(questions):\n","    print(f'Answering question: {i + 1}')\n","    question_embeddings = np.array([get_text_embedding(question)])\n","    D, I = index.search(question_embeddings, k=2)\n","    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n","    prompt = f\"\"\"\n","        Context information is below.\n","        ---------------------\n","        {retrieved_chunk}\n","        ---------------------\n","        Given the context information and not prior knowledge, answer the query.\n","        Query: {question}\n","        Answer:\n","        \"\"\"\n","    output = chatAgent(prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=30, top_p=0.95)\n","    output = output[0]['generated_text']\n","    predicted_answers.append(output)\n","\n","end = time.time()\n","print(f\"Time taken to answer questions on E5-small-V2: {round(end-start, 2)}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QCxuhDBwNhKW","outputId":"da07d63b-e494-4e4b-b52e-f4695d5358e6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Answering question: 1\n","Answering question: 2\n","Answering question: 3\n","Answering question: 4\n","Answering question: 5\n","Answering question: 6\n","Answering question: 7\n","Answering question: 8\n","Time taken to answer questions on BGE-small: 1086.67s\n"]}],"source":["predicted_answers = []\n","\n","start = time.time()\n","for i, question in enumerate(questions):\n","    print(f'Answering question: {i + 1}')\n","    question_embeddings = np.array([get_text_embedding(question)])\n","    D, I = index.search(question_embeddings, k=2)\n","    retrieved_chunk = [chunks[i] for i in I.tolist()[0]]\n","    prompt = f\"\"\"\n","        Context information is below.\n","        ---------------------\n","        {retrieved_chunk}\n","        ---------------------\n","        Given the context information and not prior knowledge, answer the query.\n","        Query: {question}\n","        Answer:\n","        \"\"\"\n","    output = chatAgent(prompt, max_new_tokens=256, do_sample=True, temperature=0.1, top_k=30, top_p=0.95)\n","    output = output[0]['generated_text']\n","    predicted_answers.append(output)\n","\n","end = time.time()\n","print(f\"Time taken to answer questions on BGE-small: {round(end-start, 2)}s\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m_OVBXSGNhKW"},"outputs":[],"source":["real_answers = []\n","\n","with open('actual.txt', 'r') as f:\n","    text = f.read()\n","\n","for t in text.split('---'):\n","    real_answers.append(t)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-kxMRHVNhKW"},"outputs":[],"source":["predicted_answers_processed = []\n","\n","for answer in predicted_answers:\n","    for i, word in enumerate(answer.split()):\n","        if word == 'Answer:':\n","            predicted_answers_processed.append(' '.join(answer.split()[i+1:]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fuDpGLbRNhKW"},"outputs":[],"source":["real_answers_embeddings = np.array([get_text_embedding(answer) for answer in real_answers])\n","predicted_answers_embeddings = np.array([get_text_embedding(answer) for answer in predicted_answers_processed])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1rznj3B2NhKW"},"outputs":[],"source":["import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","cosine_similarities = []\n","euclidean_similarities = []\n","\n","for i in range(len(real_answers)):\n","    cosine_sim = cosine_similarity([real_answers_embeddings[i]], [predicted_answers_embeddings[i]])[0][0]\n","    cosine_similarities.append(cosine_sim)\n","\n","cosine_similarities = np.array(cosine_similarities)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kwnCpLcNhKW"},"outputs":[],"source":["with open('predicted.txt', 'w') as f:\n","    for answer in predicted_answers_processed:\n","        f.write(f'{answer}\\n\\n\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x34ysaCyNhKX","outputId":"4980fa7e-57a8-41b1-9fa3-44afa326d08d"},"outputs":[{"data":{"text/plain":["0.84720886"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["cosine_similarities.mean()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBzlLLxSNhKX"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}