index,question,student_answer,grades_round,ref_answer
0," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An artificial neural network is a massively parallel distributed processor with simple processing units that has the natural propensity to store experiential knowledge and make use of them. An artificial neural network is similar to the human brain in two ways: 1. The ANN works by the process of learning from its environment. 2. Interneuron connections called synaptic weights are used to store the knowledge gained.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
1," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",Artificial neural network consists of: . Largely parallel distributed processor . simple processing units . that has ability to store the experential knowledge and making it available to use It resembles to human brain in two ways: . Knowledge is acquired from the environment by the network as learning process . Synaptic strengths called weights are used to store the knowledge,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
2," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An artificial neural network is a massive distributed processor. It consists of several information processing units which are able to acquire and store knowledge.,1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
3," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An ANN is a layered graphical model containing neurons and weighted connections, resembling the excitatory properties of the human brain. Weights of the ANN are changed after presenting it training examples from an environment, where weights are changed based on the training procedure used. Artificial neurons also are biased, just like real ones, adding a constant level of activation before being activated by a (nonlinear) activation function. Depending on the training procedure, both weights, topology or even activation functions may be learned.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
4," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial Neural Networks are large parallel processing units that have the natural ability to learn experiential knowledge. They are composed of interconnected neurons as basic units; which in turn cosists of weights, squashing functions and adder functions. ANN resembles brain in the manner that like in human brain, it is composed of a network of neurons which help in learning by adjusting the synaptic weights of the connections between neurons. This enables it to learn experiential knowledge.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
5," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An articial neural network consists of neurons. Each neuron can have several weighted inputs, an activation function and output. Usually several neurons are connected together. Often in layers. The network then calculates the output given an input to the network. The human brain works in a similar way. It also consits of neurons that are connected in several ways.",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
6," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An ANN is a - massivly parallel distibuted Processor - made up of simple processing units - which have the capability of storing experiantal knowlenge - and is made up for use. An ANN resembles the brain because: 1) it gets its knowlenge through a learning process from its environment. 2) it stores its knowlenge in its interneuron connections (synaptic weights),2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
7," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",A ANN is a massively distributed processor. It has the propensity to store experiental knowledge and make it available for use. The knowledge is gained throug a process of learning. The knowledge is stored in the weights between the neurons. This structure resembles the structure of the brain. Neurons are a the basic information unit in the ANN and act similiar to real neurons.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
8," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An artificial neural network is defined as a learning machine which is divided by layers and each layer is composed by neurons. The neurons from different layers can be connected between each other, and give an output or multiple outputs by a given input. This structure is very similar with the neurological structure of our brain, where neurons are interconnected by synapses. Also important to mention, if a feature is really important for a given task, this wil have more connections and neurons participating (like in the human brain, the important humasn functions have more synapses).",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
9," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An artificial neural network is a graph of small and identical processing units that these small units called neurons and they are connected to each other in different architectures and the whole network adapt and itself to the environment inputs by trying to decrease the error or the cost function and increase its preciseness by manipulating the free variables of the network which are the synaptic weights. It is similar to human brain because similar to the human brain we have many small processing units that are connected together and they react to the environment and learn from the environment.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
10," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial neural network is highly parralel processing. It has a mathematical model similar to human brain, which it was inspired from, as human brain does computation in an extremely parallel manner. Similarities also lay in terminology, ANN is using neurons that are smallest computing unit of a network, similarly to human brain.",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
11," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","It is a massive parallel distributed processor made up of smaller processing units, that aquire knowledge through the environmnet through a learning process and makes it available for use. It resembles the brain in two ways: - Knowledge is aquired through a stimulating process in the environment - The knowledge is embedded in the synaptic links (weights) of the neurons.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
12," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","ANN is a learning machine which is composed of neurons as units of computation. The ANN learns via interacting with its environment. The ANN has built-in capacity to dynamically adapt upon input stimulus. The ANN is motivated from biological brain and resembles human brain in terms of its localized representation for the inputs. In terms of motor cortex, the sensory stimulus to diffrent body-parts activates local part of the brain, similar to ANN local representation of similar type of input.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
13," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","A neural network is a massively parallel distributed prcoessor made up for simple processing units that has a natural propensity for storing experiential knowledge and making it available for use. It resembles the brain in two respects * Knowledge is acquired by the network from its environment through the learning process * Interneuron connection strengths known as synaptic weights, are used to stor the acquired knowledge.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
14," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial neural network is a massively parallel distributed processor which consists of one or more processing unit called neuron. It resembles the human brain for that it acquires knowledge from the environment through learning process, and that the acquired knowledge is stored in the synapses.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
15," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Definition: 1. Artificial neural networks are massively distributed parallel processor. 2. It is made up of small units, 3. Which has the propensity for storing the experential knowledge. 4. And making it available for use. It resembles the brain in 2 aspects. 1. Similar to the brain, artificial neural network does the process of learning from the environment. 2. It as a pair of inter neuron links known as the synaptic weights, which is used for storing the information.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
16," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",Artificial neural network is massive parallely distributed processor. It comprises of small processing units called neurons. It learns from experiencial knowledge which is then stored and can be used for making predictions. It resembles human brain in 2 ways: * It learns from experiencial knowledge * Knowledge is stored in synaptic interneuron connections.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
17," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","HERE: Artificial neural network is a massively distributed parallel processor which is composed of simple processing units called neurons, which have the natural propensity for storing experiential information and making it available for use. It resembles the human brain in the following aspects. - Knowledge is acquired by the network from its environment through a learning process. - Synaptic links are used to store the acquired knowledge.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
18," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","ANN is a learning machine which can perform complex parallel computation. It has the ability to learn through the interactions withthe environment and store the learned knowedge. It resembles the human brain in performing complex learning tasks, acquiring information, apadpting to the environment, and exploiting the acquired information.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
19," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An artificial neural network is a massively distributed parallel processor made up of simple processing units that have the natural propensity for storing experiental knowledge and making it available for future use. It resembles the brain in the following ways: 1. Artificial neural networks have the ability to acquire knowledge from the environment in which they are are embedded. 2. Inter-neuron connection strenghts called synaptic links activate each neuron during the learning process.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
20," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An Artificial Neural Network is a massively parallel distributed processor which interacts with its surrounding environment, with a propensity to store knowledge and make it available to use. It resembles the brain in two aspects: 1. It has the ability to learn from its environment 2. The knowledge is stored in synaptic weights",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
21," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial neural network is massively distributed paralled processor containing simple processing units and has natural propensity to store experiential knowledge and use it.It resembles the human brain in two aspects, it gains knowledge from the environment and adapts the synaptic weight to store the knowledge.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
22," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","It is a massively parallel distributed processor consisting of simple processing units, which can store experiential knowledge and make it available for use. it resembles the human brain in 2 ways: 1. knowledge is acquired from environment through a learning process; 2. interneuron connections are used to store the experiential knowledge.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
23," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",Artificial neural network is a massively parallel distributed processor that is made up of simple processing units called neuron. It can replicate human brain by storing information in their weights,1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
24," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial neural network is a **massively parallel distributed processor** with synaptic links that can able to **store experimental knowledge** and make it available for use. It resembles human brain in two ways, * Knowledge is acquired by the neural network from its environment through learning process. * Interneuron connection called synaptic links stores the acquired knowledge.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
25," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial neural network are the network of the units that learn data from the environment and store them using synaptic weights. The structure of the artificial neural network is similar to human brain. It has neurons, ie., the store units and the axoms called synapses which link the stored data.",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
26," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",Artificial neural network is massive parallel processor made up of simple processing units called neurons. They are capable of storing experential knowledge and make it available for later use. Similarity to human brain: 1. they learn from the envirnoment 2. they store knowledge as synaptic weight in the interneuron connection,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
27," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An artificial neural network is a highly distributed processor which consists of several simple processing units. It resembles the human brain, because the processing units are neurons, which are connected with weights. The human brain also consists of neurons.",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
28," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","A massively distributed processor, consisting of single processing units that have a natural prospensity of storing experimental knowledge and making it available for use.",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
29," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An artificial neural network consists of neurons, which are small computation devices,and synapses, the connections between the neurons. This resembles the brain because it also has neurons and synapses. Also a artificial neural network has weights, which are used to store learned features from the environment. Like the brain a neural network learns from the environment. An artificial neural network also has an activation function, which creates the output.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
30," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An artificial neural network is a highly parallel computation model with learning and memory capacities. Similar to the brain it learns from the environment by strengthening the synapses between neurons. Once a task is learned it can be quickly used by reactivating those learned synapses.,1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
31," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","An artificial neural network is a highly parallel working machine which consists of simple processing units (neurons) wich are connected to each other in layers. they are function approximators the brain is resembled in the architecure, the processing units and thge weights and how the learning process takes place and the properties of the brain: fault tolerance, parallel computing, ...",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
32," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An ANN is a massivly parrallel distributed learing machine made up of small computational units. Computational units are connected via synapses defined by a weight. It resembles the human brain in two aspectes:,1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
33," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","Artificial neural network is massively parallel distributed processor made up of simple computing units called neurons which aquires knowledge from environment through learning. It resembles brainlike structure in two ways, 1. It aquires knowledge through learning and experience 2. It stores knowledge in interneuron connections called synapses.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
34," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","ANN is huge parallel distributed processor , consist of simple processing units and which has propensity of storing experintial knowlegde and making it available for use.",1,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
35," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",Artificial neural network is a massively parrallal distributed processor made up of simple processing units which has a natural propensity to acquire knowledge from the environment and make it available for future use. It resembels the human brain in following ways. 1. Both of them acquire knowledge from the environment. 2. The neurons are connected by synapses cahrecterized by their weights which can be adjusted.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
36," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!",An artificial neural network is a massively distributed parallel processor made of simple processing units. It has natural propensity to store experential knowledge and it makes the knowledge available for further use. An artificial neural network uses inter neuron connections called synaptic weights to store the knowledge acquired knowledge which is very similar to how human brain works.,2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
37," Give a definition for the term ""artificial neural network"" and mention, how it resembles the human brain!","ANN is a massively distributed processor, consisting of simple processing units called neurons. These neurons in terms of ANN are similar to neurons in human brain. Both neurons are characterized by synapses(connection links). They represent connections used for data flow between neurons. In both ANN and Human brain, the knowledge is represented by its very structure and activation state of neurons.",2,"A neural network is a massively parallel distributed processor which is made up of simple processing units. It has a natural propensity for storing experiential knowledge. Neural networks resemble the brain in two aspects; knowledge is acquired by the network from its environment through a learning process, interneuron connection strength known as synaptic weights are used to store the acquired knowledge."
38,"Define the mathematical model of a neuron, use the appropriate technical terms!",A neuron is the simplest processing unit of a neural network which has: 1. synaptic weights to store the knowledge gained. 2. Adder function (linear combiner) which adds the weighted values of the input signals to produce the local field. 3. An activation function which squashes the local field to a range of values. $ \phi(\sum{i=0}^{N} wi \cdot xi) $,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
39,"Define the mathematical model of a neuron, use the appropriate technical terms!","Mathematical model of a neuron is given as : y = $\phi(V)$ , where activation function is applied to local field(V) V = $\summation (w{i}x{i} + b)$ . Local field is weighted(w) sum of inputs(x) plus bias(b)",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
40,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron is an information processing unit. It consits of: inputs associated with weights, sum of inputs and an acitvation function",1,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
41,"Define the mathematical model of a neuron, use the appropriate technical terms!",Input vector $x$ Weight matrix $w$ Net input $net=\sum x^Tw$ Net output $o=\phi(net)$,1,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
42,"Define the mathematical model of a neuron, use the appropriate technical terms!",A neuron consists of three basic components: - *Synaptic Weights*: The synaptic weights are connections between neurons and are adjusted through training. - *Squashing/Activation Functions*: The squashing functions may be non linear or linear functions that that are applied to the signals from the neurons - *Adder Functions*: The adder functions help in combining outputs from several neurons.,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
43,"Define the mathematical model of a neuron, use the appropriate technical terms!","$N$ number inputs, $xi$ input i, $vj$ local field, $\varphi(vj)$ activiation function, $yj$ output, $w{ji}$ weight from node i to j $yj = \varphi(vj)$ $vj = \sum{i=0}^{N}w{ji}xi$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
44,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron is a simple processing unit of an ANN, that is made up of - the synaptik links which are defines by a weights $(w1,...,wn)$ - a adder function that combines the weighted input $(wi*xi)$ plus some bias $(b)$ to the local field $(\sum{wi*wi}) +b=v$ - a activation function phi that squaches the local field to the output $(phi(v)=y)$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
45,"Define the mathematical model of a neuron, use the appropriate technical terms!",The neuron consists of synapses/connecting link each characterised by a weight. A linear combiner sums up the weighted sum of inputs to a local field. The local field is then passed through an activation function. The result of the activation function is the output.,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
46,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron is defined by the following elements: - A number of input values x - A number of weights w - A bias b - An activation function $/phi$. The inputs x are multiplied with the weights, and the result is summed with the bias (also, the bias can be used just as a weight value b and a single connetion with an stable input equal to 1, for mathematical simplicity). The resulting value, known as local field (v), will be the input to the activation function. The mathematical model can be summarized in the formula: $v = \sum^{n}{i = 0} x(i)*w(i) + b$ $y = \phi(v)$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
47,"Define the mathematical model of a neuron, use the appropriate technical terms!",A neuron consists of a set of inputs and a bias which these inputs and predefined bias will be multiplied by a weight and then we have sum the results of all the inputs and bias multiplied by the weights which called induced field and after that we send this to an activation function which can be a linear or non-linear function and the output of this function is the final output of our neuron.,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
48,"Define the mathematical model of a neuron, use the appropriate technical terms!","Neuron is a simplest computation unit of a neural network that consists of input variables, weights, bias, summation term (combiner), activation function and output variables.",1,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
49,"Define the mathematical model of a neuron, use the appropriate technical terms!","The neuron is the basic processing unit of a neural network and is made of three main component: - Weights: $w1, w2, ...,wn$ - Adder function: it is the linear combination of the input and weights plus bias. (induced local field) $v = \sum wi xi + b$ - Squashing function: it is the activation function applied to the local field used to limit the output of the neuron. $\phi(v)$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
50,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron is a computational unit composed of + synapses which are stored in the form of weights $w$. These are the variables that can are dynamical. + summing function that computes the weighted sum of inputs: $v = \sumi (wixi)$ + activation function $\phi$: gives nonlinear nature to network, determines and normalizes the output produced by neuron. e.g. sigmoid function + bias: another synaptic tunable variable with input 1. Therefore the net output of neuron: $ y = \sumi (wixi) +b$.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
51,"Define the mathematical model of a neuron, use the appropriate technical terms!","The following equations describe a nonlinear model of a neuron, labeled k. 1)uk = sum from j=1 to m w{kj} x{j} 2)yk = phi(u{k} + b{k}) where x{j} are the input signals; w{kj} are the weights of the neurons; u{k} is the linear combiner output due to the input signals; b{k} is the bias; phi() is the activation function; and yk is the output signal of te neuron.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
52,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron is a processing unit that contains three main components: a set of synaptic weights that connect the neuron with other neurons; an adder that computes the induced local field, or the weighted sum of the signals flowing through the neuron; an activation function that constrains the magnitude of the output signal from the neuron.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
53,"Define the mathematical model of a neuron, use the appropriate technical terms!","A Mathematical model of a neurons consits of a 1. A set of synaptic links which are classified based on weights(w1, w2, w3...wn) 2. It consits of a adder function, which performs the weighted sum of the inputs and the bias. $\Sigma{i=1....n} wn.x + b$ 3. It consists of an activation function, used to minimize the amplitude of the neuron output. $\Phi(\Sigma{i=1....n} wn.x + b)$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
54,"Define the mathematical model of a neuron, use the appropriate technical terms!",A mathematical model of neuron comprises of 2 main units: * Adder functions: it sums up all the product of all synaptic connections and inputs of neuron * Synaptic weights: these are interneuron connections in which the knowledge is stored * Activation function: it is used for introducing non-linearity,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
55,"Define the mathematical model of a neuron, use the appropriate technical terms!",HERE: The neuronal model consists of the following: - Synaptic links characterized by their weights which connects the network to the environment it is embedded in. - An adder function which sums up the weighted inputs and outputs the induced local field of the neuron. - An activation function which takes the induced local field of the neuron as it's input and limits the output of the neuron.,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
56,"Define the mathematical model of a neuron, use the appropriate technical terms!",A mathematical model of neuron consists of 3 important parts. A neuron is the smallest computaional node with: 1) Input vectors : set of vectors of a certan dimension to train the model 2) Weights (and biases): each of the input vectors are weighted using weight vectors in accordance withthe output that is required. Bias is added when necessary. 3) Activation function : The linear combination of weights and inputs are passed through the activation function which produces an output.,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
57,"Define the mathematical model of a neuron, use the appropriate technical terms!","The neuron is the fundamental processing unit of an aritificial neural network that is characterised by the followig features: 1. A neuron has a set of non-linear synaptic links, an externally applied bias, and possibly one or more linear activation links. The bias is represented by a synaptic link from an input fixed at +1. 2. The synaptic links of the neuron weight the respective inputs. 3. An adder function (linear combiner) computes the weighted sum of the inputs to the neurons. 4. An activation function (squashing function) limits the amplitude of the neuron's output.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
58,"Define the mathematical model of a neuron, use the appropriate technical terms!","Let $x1$, $x2$, ... , $xN$ be the inputs to the neuron, $wi$ be the corresponding weights of connections, $b$ be the bias and $\varphi(.)$ be the activation function. Then, the induced field $v$ is given by - $$v = \sum{i = 1}^{N} wi .xi + b$$ The output $y$ is given by - $$y = \varphi(v)$$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
59,"Define the mathematical model of a neuron, use the appropriate technical terms!","The mathematical model of neuron has three parts: - a set of synapses or connencting links characterized by weight ,w . - an adder function that calculates the weighted sum of inputs plus some bias - an activation function (squashing function) to minimize the amplitude",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
60,"Define the mathematical model of a neuron, use the appropriate technical terms!","$vk = \sum{j=1}^{m} w{kj} xj + bk$, $yk = \phi(vk)$, $w{kj}$ is the synaptic weight connecting neuron k and input data j, $xj$ is input data, $bk$ is bias, $vk$ is induced local field, $yk$ is output of neuron.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
61,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron consists of a synapse connecting link, an adder function or linear combiner and an activation function. $$v = \Sigma wi \cdot x{i} + b$$, where $xi$ is the input, $wi$ is the weight and $b$ is bias.",1,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
62,"Define the mathematical model of a neuron, use the appropriate technical terms!",A neuron is a basic information processing unit that have a adder function to compute **weighted sum of inputs plus bias** and apply activation function on the result. $$ \phi(v) = \sum\limits{i=1}^n \omega(i)x(i) + bias $$,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
63,"Define the mathematical model of a neuron, use the appropriate technical terms!","Each neuron has a set of inputs and their respective weights. The local field is, $v = \sum(w{ij} * xi)$ The local field is passed through a activation function. So the output of the neuron is, $y = \phi(v)$ $y = \phi(\sum(w{ij} * xi))$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
64,"Define the mathematical model of a neuron, use the appropriate technical terms!",The neurons are the basic processing units in neural network output of the neuron = $ \phi (\sum w{i} x{i})$ they consist of three parts Synaptic weight: the connections between the neurons. characterised by weights Adder function: calculates the weighted sum of the inputs of the neuron Activation function: limits the amplitude of the output of the neuron. ($\phi$),2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
65,"Define the mathematical model of a neuron, use the appropriate technical terms!",The model of a neuron consists of synaptic weights which are applied to the input signals. The weighted inputs are then summed which gives the local field. This local field is put into an activation function whose output will be the output of the neuron.,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
66,"Define the mathematical model of a neuron, use the appropriate technical terms!","$y = \sum{i=0} \Phi(w*xi)$ A neuron consists of inputs $x$, synpatic weights $w$, an extra input $w0$ which is fixed to 1 for the bias, an Adder function, that creates the local field $v$ and a squashing function $\Phi$.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
67,"Define the mathematical model of a neuron, use the appropriate technical terms!","$y = \sum f(wx + b)$, where w are the weights, which change the input according to the learned weights, x is the input from the environment, b is the bias, which shifts the learned decision plane, and f() is the activation function, which limits the output to a desired region of values.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
68,"Define the mathematical model of a neuron, use the appropriate technical terms!",A neuron consists of one or multiple inputs which are gathered by a summation function. The hereby induced local field of the neuron is processed by a squashing function and generates the output of the neuron.,1,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
69,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron consist of input connection links with a synaptic weight, a bias, an adder which adds the input singnals and the bias and produces the local field. The local field is processed by the activation function and produces the output of the neuron.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
70,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron consists of input nodes x1 to xn and weights w1 to wn, a linear combiner v= SUM( $ xi * wi $) + b, where b is some bias. The result v is called local field and is used as input for an acivation function $ phi(v) $",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
71,"Define the mathematical model of a neuron, use the appropriate technical terms!",Neuron is consists of three units. 1. Synaptic links characterizex by weights which linearly ways the input. 2. Adder which adds weighted inputs to generate local field 3. Activation function which is nonlinear function sqashing the output of the neuron,2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
72,"Define the mathematical model of a neuron, use the appropriate technical terms!",1) Neuron is consist of sysnaptic links which measured in terms of weights. neuron is given with inputs.\ 2)it has adder funtion or combiner which adds all the inputs mulitplied by the weights and bias is extra input to the neuron as well. 3) it has a activation link which limit the amplitude of the output of the neuron.,1,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
73,"Define the mathematical model of a neuron, use the appropriate technical terms!","Neuron is the basic information processing unit which is the main component of a neural network. A neuron is charecterized by its input ($xi$), synaptic weight ($wi$) and activation function $\phi(v)$. Mathematically it can be modelled as $\phi(wixi)$. Activation function bounds the input to a certain level.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
74,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron has three components * Synaptic weight: w * Adder function: it multiplies input x with the weight * Activation function: It squashes the output of the adder function. Sigmoid, hyperbolic tangent, Rectified linear unit etc.",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
75,"Define the mathematical model of a neuron, use the appropriate technical terms!","A neuron consist of set of inputs that takes data from environment. Each neuron contains synapses(connection links) that are characterized by weights. All inputs are connected to the summing (adder) function, that computes weighted sum of all input values. This weighted sum is called local field of neuron. The value of this local field (V) is limited(squased) by an activation funtion $\theta(V)$. The result from this squasing funtion is output of a neuron ($y = \theta(V)$). Additionally, a bias term $(b)$ is added to the input, and its value is always 0, but its associated weight is being changed over training period. Finally, output of neuron is $y = \theta(V)$, where $V = \sum Wj * Xj + b$",2,"Mathematical model of a neuron consists of a set of synapses or connecting links where each link is characterized by a weight, an adder function (linear combiner), which computes the weighted sum (local field) of the inputs plus some bias and an activation function (squashing function) for limiting the amplitude of a neuron’s output."
76,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","1. Label one class a positive with label +1 and the other class as negative with -1. 2. Augment the data with an additional value for the bias term. 3. Invert the sign of the data in the negative class. 4. Randomly initialize weights. 5. If $w^T \cdot x <= 0$, update weight by $ w(n+1) = w(n) + \eta x(n) $, else leave the weight unchanged. 6. Continue step 5. 7. terminate when there is no longer a change in any weight.",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
77,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",1. Initialization: n(time step or iteration) = 1 and weights are small but randomly initialized 2. Activation of perceptron: Apply training pattern to activate the perceptron 3. Compute Output: Apply Activation function to the local field(weighted sum of inputs plus bias) 4. Adjust Weights: Adjust weight if current output(y) != desired output(d) 5. Continuation: We continue by increasing n during each iteration and repeat from step 2 untill all input pattern are applied to network and also error is minimized,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
78,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","HERE: y denotes the actual result, d denotes the desired result positive train error: y = 0, d=1 $w{new} = w{old} + x $ negative train error: y = 1, d = 0 $w{new} = w{old} - x$",1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
79,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","initialize weights with zero or small values; sample data point, feed into network; compute net output, use the step activation function; compute error $e=(d-o)$, where d is the true label, o is the predicted label; correct weights based on $w(t+1)=w(t)+\alpha(d-o)x$, where alpha is the training rate and x is the input pattern; repeat for each pattern until convergence is reached;",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
80,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","For this case, the parameters that need to be learned are the slope of the line and the intercept. These are the parameters for the weight vector. 1. Initialize random small values for weight vector. 2. For inputdata $xi$ in Training Data: - Apply the input to the weight vector. - e = the difference between the local field and the desired output $(di-yi)$ - Update weight: w(n+1) = w(n) + $\eta e xi$",1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
81,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","$\varphi(v) = \tanh(v)$, single node network, $\mu$ learning rate repeat as long as error is too high: 1. present sample to network and collect output. 2. compare actual output with desired output (d). 3. If not equal adapt weights: $wi(n + 1) = wi(n) + \mu(d-y)xi$",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
82,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","given $k$ date points $(xi,yi)$ and $yi\in\{1,-1\}$ given a learning rate for each point i add a bias 1 so that point i == (1,xi,yi) ; for each point i there yi == -1 point = -1 * point; w= Nullvector; b = 0; convergance = false; while(convergence == false) convergance = true; for each point i in the training set: if(w*x<=0) do w = w+learningrate*pointi; convergance = false;",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
83,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","weights # a weight vector phi = activation function eta = learning rate for each datapoint (xi,yi) do: weights[i] = weights[i] + eta * (xi[i]-yi)*weights[i]",1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
84,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","1: w, b = initweightsbias() // the weights can be initialized to 0 or random initialized 2: n = 0 3: WHILE !stopcriteria() DO // iteration until stop criteria is fulfilled 4: y = w(n) * x(n) + b // calculate output 5: IF x is in C1: e = 1 // if the x belongs to class C1, error i 1, otherwise is -1 6: ELSE IF x is in C2: e = -1 7: w = w + e * x // update weights using the calculated error 8: n = n + 1 9: END The stop criteria can be, if the number of misclassified input data is 0, then stop.",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
85,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",The learning process consists of three main steps: 1- Positive error: - calculate the error of all the data sets in the learning set - change the w(weight): w(n+1) = w(n)+positive error - seperate the data points based on the new w 2- Negative error: - calculate the error of all the data sets in the learning set - change the w(weight): w(n+1) = w(n)+negative error - seperate the data points based on the new w 3- No error: - when we have no error this is the end of the training,1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
86,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","Define a bias in order to be able to trigger to which class data points will be classefied to. Assign initial randomly chosen weights, use a squashing function for example McCullon Pits, start training proccess and stop when error of output and desired output has reached desired percentage.",0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
87,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","Initialize the weight vector $\hat{w} = 0$ - do - for every training sample x,d $v = \sum wi xi + b$ $y = \phi(v)$ if d is not equal to y then $e = d - y$ $w = w + \eta e(i) xi$ - until convergence",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
88,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",**Pseudo code** + initiate weights and bias randomly. + compute output for the given input data $ y' = \sumi (wixi) +b$. + compute error between computed $y'$ and desired output $y$. + update weights: $w(n+1) = w(n) + \eta (y-y') x$ + stop when the error is below some specified threshold or becomes zero in case of data that is perfectly linearly separable.,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
89,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",,0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
90,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","Initialize the perceptron with each weight equal to 0: $w(0) = 0$. Present the labeled examples $(xi, di)$ to the perceptron. > for each example $(xi, di)$ >> Compute actual output $yi$ and error signal >> Update weight based on the dlelta rule: $w(n+1) = w(n) + \eta (d(n) - y(n)) x(n)$",1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
91,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",We use threshold function as activation function. if w.x + b >= 1 label class 1. else label class 0.,0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
92,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",e(n) = current error <br> eps = convergence criteria <br> n = learning rate <br> while (change in e(n) not less then eps) {<br> calculate error e(n) <br> w(n+1) = w(n) + n e(n) x(n) (Widrow Hoffmen rule) <br> },2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
93,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","HERE: Perceptron learning algorithm: - Initialize the network by assigning random weights to the synaptic links. - Calculate error as the difference of the desired output with the actual output. - If the input is misclassified with positive error, $w(new)$ = $w(current) + input$. - If the input is misclassified with negative error, $w(new)$ = $w(current) - input$. - If the input is correctly classified, no changes are made in the weights. - Repeat from step 2 as long as the error is under some defined threshold value.",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
94,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","The linear binary classifiable data consists of input vector $X$ which when multipled with weights and added bias, fall into class+ or class- depending on the linear combination output of $WX + b$ being above 0(+) or below 0(class -). Algo: Para,meters : X,Y(desired output), W, b 1) weight vector W is initialized with small random values. 2) Input vector is chosen with a probabiity and output is computed using $WX + b$ . If the class Y of vector X is + and output is $<0$, or if the class of X is - and output if >0, then the weights are updated accordingly. Otherwise weights are left unchanged. 3) iterated over other input vectors until convergence of output.",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
95,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","1. Initialization : At time step n(0), initialize weight vectors with random values $wj(0)$ 2. Activation : Apply the input example $(xi(n),di(n))$ to activate the perceptron with heavyside step function as the activation function. 3. If output of the perceptron $y(n) \neq d(n) $, adjust the weight vector using the rule : $w(n+1) = w(n) + \eta x(n)(d(n) - y(n))$ 4. Go to Activation and repeat until no more change in weight vector is observed",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
96,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","1. Inputs X: $x1$, $x2$, ... , $xN$ 2. Desired outputs y: $y1$, $y2$, ... , $yN$ 3. Initialize weight vector $w$ to random small values 4. For each data point $xn$ in X: Calculate $\hat{y}n$ from $w$ and $xn$ Calculate error $en = yn - \hat{y}n$ Update $w$ according to delta rule end",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
97,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",,0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
98,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",apply input data to input layer and initialize small values weights minimize error according to difference between desired signal and output signal assign the test vector the class that has smallest error,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
99,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",1. Compute the initial weights for all input vector 2. Apply matrix multiplication from input to weight vector 3. Apply linear combiner 4. Apply activation function to produce the output 5. Compute the error 6. Update weights,0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
100,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","* Randomly assign values to initial weights * Run the perceptron network and calculate the error (e = y-d) where, e is error, y is output and d is desired response. * Update the weights based on the error. * If error is positive, add the error with the input and update weight. * If error is negative, subtract the error with the input and update weight. * If there is no error, don't update the weights. * Repeat the above process until the calulated error is approximately equal to zero.",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
101,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",,0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
102,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",w = [random number betrween -1 and 1] for every data in training set { In the first layer: calculate the weighted sum using adder function calculate the output of the activation function In the ouput layer calculate the output y calculate the error e = d - y ; d- desired output change the weights using the formula $ \delta w = \eta xj ej$ } continue till the error converges,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
103,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",Initialize as many random weights as the dimension of the data points For each data point: if the output matches the desired output do nothing else: change the weights in the direction of the datapoint so that the datapoint is classified correctly end if end for if some weight was changed: start again with the for loop end if,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
104,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",1. Initialize the weights at random or as 0. 2. Activate the Perceptron by giving an example. 3. Compute the actual output of the neuron. 4. Adjust the parameters of there perceptron. 5. Continue until convergence is achieved. w = rand y = sum($\Phi$(w*x)) for wi in w: wi = wi+$\eta$*e*y,1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
105,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","Initialize the weights randomly. $y = \sum f(wx + b)$, compute the output of the perceptron using the input x, the weight w, the bias b and the activation function f(). $e = d - y$, calculate the error by substracting the actual output from the desired output. $w{new} = w{old} + learning\rate \cdot x \cdot e$, update the weights with this formula. The learning rate is a parameter which changes how fast the perceptron learns.",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
106,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",for n iterations for each datapoint d error = desired - output if error > 0 weights = weights - error if error < 0 weights = weights + error,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
107,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",pick random decision boundary while one of data points is in wrong class turn decision boundary by using vector of wrong data point (negative rule or positive),1,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
108,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",trainingset := set of labeled linear seperable data points w := weight vector with dimension of input data v := local field phi(v): activationfunction (threshold function) y:= output e := error (y - d) where d is the desired output from labeled training data n := learning rate (0.1) assign random values for w for x in trainingset: v = sum(xi * wi) y = phi(v) e = y - d w = w + n*x*e // delta rule end,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
109,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
","initialize weights w and bias b set learning rate n set errorthreshold (upper bound on error) while error < errorthreshold : for every datapoint x in tarining dataset : y = [w, b] . [x, 1] (bias is represented as weight of fixed input 1) if y is positive then x belongs to C1 otherwise to C2 store above predicted class. find error in predicted output with respect to the labels store error e esum = sum of all errors e for every data point w = w + n * esum",2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
110,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",for a binary classifier we can use threshold activation funtion. 1) randomly initilize the weights 2) you calculte the output of the neruon 3) find out the error by subtracting expected output and current output. 4) modify the weights related to that input with respect to the error. 5)repeat the process 2-4 till the you get minimal error.,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
111,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",,0,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
112,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",continueprocess = true w = randomlyinitialize() while continueprocess for x in list of points y = w.x diff = d-y // d is the desired output if(diff >= 0) w = w + x else w = w - x if all points are classified without error continueprocess = false,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
113,"Assume you want to binary classify linear separable data point using a perceptron. Write down the learning algorithm in pseudo code!
",n<- learinig rate Repeat until the MSE is small enough: t=t+1 for each point in training set do: compute local vield of percepron: V = W*X apply linear activation function: $y = \theta(V) = V$ compute current error: e = (d-y) apply delta rule: W(t+1) = W(t) + n*e*X end,2,"Label the data with positive and negative (+/-) labels, initialize the weights randomly, apply (simplified) update rule: Dw = eta*x(n) if <w,x> <= 0, repeat on all epochs till the weights don’t change much. The algorithm will converge as the data is linearly separable.  "
114,Explain classification and regression; what is the difference?,"Classification: In classification, the output produced by the NN is a discrete value which indicates which class the input belongs to. Regression: In regression, the output produced by the NN is a continuous variable. This could be used for instance, to approximate a continuous function.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
115,Explain classification and regression; what is the difference?,"In classification, output values are always discrete. In regression, output values are continuous",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
116,Explain classification and regression; what is the difference?,"A hyerplane is given by y = w*x + b . Regression wants to determine w Classification wants to assign a class to a set of observations. Regression wants to determine separating hyerplane, classification wants to label data points with a class",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
117,Explain classification and regression; what is the difference?,"In classification tasks, we assign discrete labels to data points of our training dataset, either being assigned a specific label or not (binary). For supervised learning, these datapoints are labeled with a label vector ground truth. In regression, we try to model a function which fits the data points of the training data, and thus model a function with continous values.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
118,Explain classification and regression; what is the difference?,"Classification: - It refers to classifying given data into discrete classes. - The output is discrete values. - Use for activity like pattern recognition, etc. Regression: - It refers to estimating the value of some continuous function given an input. - The output is continuous value. - used for activities like motor control, etc.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
119,Explain classification and regression; what is the difference?,In classification we try to assign classes to input data. Regression we want the network to behave like a given system/formala. This can also be a time series of input and output data.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
120,Explain classification and regression; what is the difference?,"In classification the goal is to saperates points into different classes. The outcome is a class lable. Regression trys to fit a hyperplante to a point cloud best, so that future data is representet by that hyperplane best (LMS). It trys to minimize the distance to all data points. The outcome is a countinius variable.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
121,Explain classification and regression; what is the difference?,"Both are learning tasks of a ANN. In classification the goal is to assign a class label to new datapoints. In regression the goal is to eastimate a unkown function. The only difference between both is that classification uses discrete class labels, while in regression a continuous output is used",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
122,Explain classification and regression; what is the difference?,"The approach of classification is to classify sets of input data into their correct classes (for example, used in pattern recognition). The approach of regression is to approximize to a defined function f by calculating the error between this function and the result of an algorithm. THe difference is that, the classification approach is applied to a discret data (the samples are the different points of the input space), and regression is an analogic approach where the whole function must be approximize (for any input given).",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
123,Explain classification and regression; what is the difference?,- Classification: In classification problems we have different groups of data that have some common properties and after training we want that our model can detect the class of the new sample correctly - Regression: In regression we have a series of values and we want to use the previuos values in this series and predict the next value,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
124,Explain classification and regression; what is the difference?,"Classification is a problem of destinguishing to which discrete classes input variables are to be assigned to, regression is estimation of the output, by figuring out the continuous trend of the whole dataset.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
125,Explain classification and regression; what is the difference?,"Classification if to assign a class or category to the data, while regression is when you fit the data to a function.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
126,Explain classification and regression; what is the difference?,+ Regression: learns model/function that can predict other unseen data well. Target/output is real spaced. + Classification: learns a model that classifies/maps input to a discrete target label. Targetlabel/output is binary/discrete.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
127,Explain classification and regression; what is the difference?,"Classification describes the application, in which a sample is assigned to one specific pattern of the problem. In comparison to regression is the output deterministic an not continiously. In regression the output is continuous describing",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
128,Explain classification and regression; what is the difference?,"Classification is the task of classifying the input signals into a finite number of groups, so the output is a number that indicates a certain class. Regression is the task of approximating a function by estimating the values given the input signals, so the output can be any real number.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
129,Explain classification and regression; what is the difference?,Classification: We need to predict the output data discretely. That is the output space is a discrete space. Regression: We need to predict the output data continuously. That is the output space is continuous space The main difference is the discreteness and contionousness.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
130,Explain classification and regression; what is the difference?,Classification is a problem of assigning a particular class to each data point in a given dataset. <br> Regression is a problem of fitting the given dataset on a particular hyperplane which can be used for representing the given data. It finds the hyperplane which minimises the mean square error.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
131,Explain classification and regression; what is the difference?,HERE: - Classification is a problem of assigning labels or classes to the input. The output is a discrete variable. - Regression is a problem of assigning a continuous variable to the input.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
132,Explain classification and regression; what is the difference?,Classification is a problem of catergorization into discrete classes where as regression is a problem in a continuous space where the goal is to ether minimize or maximize a cost function. Classification is the process of dividing a set of discrete inputs into classes corresponding to similar patterns such as clustering. Regression could be finding a pattern of the distribution of the data such as ftting a line.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
133,Explain classification and regression; what is the difference?,"Classification in machine learning is used to find a decision surface in the form of a hyperplane that can separate a set of input examples (or set of patterns) into their respective classes. Regression on the other hand is used to find the parameters (i.e, the weight vector $w$ and the bias b) for the function thatcan best fit the given data points $\{xi,di\}$ . Thus classification deals with predicting the class label for discrete data points whereas regression deals with fitting a continuous real valued function.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
134,Explain classification and regression; what is the difference?,Classification is separating the data into classes and the output is a discontinuous variable. Regression is fitting a model and the output is a continuous variable.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
135,Explain classification and regression; what is the difference?,"Classification is about classifying the given data into different classes, where as regresssion is about finding the local/global minima.We use perceptrons to classify the data and we use unconstrained optimization techniques like newton's method to find regression.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
136,Explain classification and regression; what is the difference?,classification: assign a test data to a class that is prescribed regression: approximating an unknown function with minimization errors for input-output mapping.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
137,Explain classification and regression; what is the difference?,"Classsification: In classification, the output variable takes class labels or identifying group membership<br> Regression: In regression, the output variable takes continuous values or predicting a response",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
138,Explain classification and regression; what is the difference?,Classifaction problem is used to classify set of data points into specific groups. Regression is used to predict time series data. Classification works on discreate set of values and regression works on continuous values.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
139,Explain classification and regression; what is the difference?,Classification: Classification is done between the classes. The machine determines to what class the data belongs to. Regression: Regression is a expecting output for an input. The machine learns from the given data and models a function and when new input is given it expects the output. Difference: Classification is discrete output where as Regression is a continuous output.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
140,Explain classification and regression; what is the difference?,Regression: Tries to fit a line are curve among the given points The have continuous output the output is a function Classification: Tries to classify the given points into two or more calsses They have a discrete output the output is a value representing the class,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
141,Explain classification and regression; what is the difference?,"Classification: Each datapoint is assigned with a class Regression: Each datapoint is assigned with a value In classification we assign classes or labels to datapoints. The error signal here can be only true or false. In regression we try to learn a function, the error for each prediction can be a number.",2,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
142,Explain classification and regression; what is the difference?,"In classification a binary pattern has to be partitioned into the two classes. In regression a line has to be fitted closest to some datapoints. The difference is, that in Classification mthe output is a single class label, while in regression the output is continuous",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
143,Explain classification and regression; what is the difference?,"In classification the input data is split in 2 or more classes. The goal of the neural network is to learn the input data and then be able to classify new input data into the classes. Based on the learned information the network then maps input data into one of the classes, which is discrete space. In regression the input data is learned aswell. But here the network tries to predict feature values, which are in continuous space. The network tries to predict close as possible to new input data only using the learned model.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
144,Explain classification and regression; what is the difference?,"Classification tries to label discrete data points with distinct classes, while regression tries to approximate a continuous function from discrete data points. Results of these methods are respectively a labeled data set or a continuous function.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
145,Explain classification and regression; what is the difference?,In classification the task is to give an discrete output value to an input. It assignes one of all defined classes to the current input. Regression try to approximate a function while minimizing error and produces a continous output value.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
146,Explain classification and regression; what is the difference?,"Classification means mapping inptut data a class label, for example 1 and -1. IN regression on the other hand a continuous function is learned in way that f(x) - F(x) is minimized, where f(x) is the function learned by a learning machine and F(x) is the original function.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
147,Explain classification and regression; what is the difference?,"Classification is supervised learning where underlying function representing the trining data is learn from training data to predict classes of datapoints or patterns drawn from similar distribution as of tarining data. Weights of the neural network are learned to minimize the error in classification. Regression is supervised learning algorithm where underlying function representing the trining data is learn from training data to predict the value of label or output of some system for new datapoint or pattern of similar type. Weights of the neural network are learned to minimize the error in prediction of function. Differences. : 1. Output of classification is discrete ( Class 1,2,3 ) whereas output of regression is continuous 2. Error in classification is number of wrong classifications whereas Error in classification regression is distance between lable value and predicted value",2,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
148,Explain classification and regression; what is the difference?,"classification is type of problem where algorithm needs to saperate the one data class from the another data class. If there is 2 classes C1 , C2 . algorithm classify the given data into these two classes. it is discreet process. Regression is the pridicting the next point depending on the previous points. it is continuous process.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
149,Explain classification and regression; what is the difference?,Classification is the problem where the input data has to be put in two or more classes distinctively different from each other. For example in case of binary classification on class can be -1 and the other +1 Regression on the other hand is data fitting. THe main aim is to find a hyperplane which can fit a given input pattern.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
150,Explain classification and regression; what is the difference?,Classification: Is a task to partition the given input into one of several classes. The calsses are descrete values. Regression: Regression is the tasks of predicting output in a continuous range. The prediction can be any value within a range.,1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
151,Explain classification and regression; what is the difference?,"In classification task the aim to separate data in different classes, such that output of NN gives value of class index for each input point. E.g in the task is to classify binary data, then the output of the NN will 0 or 1, and each value, represent on class. In case of regression task, the aim is to fit data, namely a function that perform input-ouput mapping. Output of NN in this case, will be error value, such that we know how close is out function fitted to data points.",1,Classification is a task of mapping data to discrete labels while regression is a task which maps data to a continuous function or real values. Error in classification is the number of misclassifications while in regression is the summed distance between the true and predicted values. 
152,Write down the SOM learning in pseudo code.,"1. Arrange the weights in the required topology according to the problem. 2. Initialize the weights randomly such that all the weights are different. 3. Sample the input from the input space. 4. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. 5. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and reduce the learning rate and make sure learning rate is above zero. 6. If ordering and convergence is complete, stop. Else continue to step 3.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
153,Write down the SOM learning in pseudo code.,"i. First we initialize random weights for neurons ii. Then we choose random input from input space iii. We compute distance between input vector and each weight vector. iv. Neuron that have minimium euclidean distance with input vector is considered as winner neuron v. Then, we find the neighborhood neurons of the winning neuron vi. We adjust the weights of all neighborhood neurons vii. Reduce the learning parameter and neighborhood size viii. Continue until it converges.",1,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
154,Write down the SOM learning in pseudo code.,"w denote weights t denotes threshold h denotes the neighborhood function, which decreases with distance d from winning neuron h(x, x{win} //neighborhood function return (exp(-2/||x-x{win}||) w = rand(); //initialize weights with random value while (w{delta} > t){ //proceed until there are no notieable changes x{win} = arg min ||x-w||^{2}//determine x which is closest to w (competetive learing) // Update weights of winning neuron // weights of losing neurons are not updated w{new} = w{old} + x*h(x, x{win})*(x-w)//update weights of neuron which a are in neighborhood of winning neuron }",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
155,Write down the SOM learning in pseudo code.,"initialize weights with small values (such that all of the weight vectors are different); sample a datapoint, feed into network; determine the winning neuron on the lattice, picking the neuron with the least euclidean distance of its weight vector to the input vector; determine the neighbourhood of the winning neuron through the neighbourhood function; change weights of the neurons, namely spatially 'pulling' the weight vectors of the neighbourhood neurons towards the input vector; depending on the timestep, reduce learning rate and neighbourhood size based on wether we are in the organizing or finetuning step; repeat until maximum number of steps;",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
156,Write down the SOM learning in pseudo code.,"1. Randomly initialize weights. 2. Randomly select an input from the training data. 3. Find the nearest neighbour of this input in the weights. This is done by finding the euclidean distance of the input from each weight. And selecting the weight with least distance. 4. Update the weights of all the neurons within the neighbourhood $h(n)$ (which is gaussian function, with an exponentially decaying $\sigma(n)$) of the winning neuron with some learning rate $\eta(n)$. $$\Delta w{ij}=\eta(n)h(n)(||xi-xj||)$$ where, $$\eta(n)= \eta0e^{-n/T1}$$ and $$\sigma(n)= \sigma0e^{-n/T1}$$",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
157,Write down the SOM learning in pseudo code.,"In SOM we start with randomized weights, $\mu$ learning reate, $d{ji}$ distance between j and i, $h$ neighbour function repeat as long as error is too high/max iterations are not reached: 1. take input sample 2. find closest node/weight 3. find all it neighbours 4. move the weight and its neighbours closer to the given input, use the neighbour function (e.g. gaussian) to reduce effect to far distance neighbours 5. (optional) adapt learning rate and neighbour function",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
158,Write down the SOM learning in pseudo code.,given a neigbourhood function $h{ij}(n)$ and a lerning rate over time randomly assing different weights from the input layer to the neurons in the second layer for each training point xi do: - find the winner-takes-all neuron $k$ with $min ||xi-wi||$ - find the neighbours of $k$ with the neigbourhood function - compute the new weights for those neurons using the neighbourhood function and the learning rate - update (decrease) the neighbourhood function and the learning rate end,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
159,Write down the SOM learning in pseudo code.,,0,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
160,Write down the SOM learning in pseudo code.,"1: w = initweights() // equal to zero or random initialized 2: n = 0 3: WHILE !stopcriteria() 4: winnerneuron, y = (x, w) // find on the map layer which neuron is closer to the input (euclidean distance) 5: neighborhood = defineneighboor(winnerneuron, n) // define the neighborhood size (first iterations big, and being reduced) 6: eta = definelearningrate(n) // define the learning rate (large value at the first iterations and being reduced) 7: diffw = adaptweights(neighborhood, eta) // adapt the weights just for the winner neuron and its neighborhood 8: w = w + diffw // update the weights 9: stopcritera = muststop(y, x) // look if the distance between input and the winner neuron is 0 (or really close to 0) 9: END",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
161,Write down the SOM learning in pseudo code.,- randomly define some values for the synapitc connections in the network - send the first input to the network - in the output layer(map layer) select the neuron that has lowest error(competition phase) - based on a predefined method define the neighborhood of the selected neuron(cooparation phase) - change the weights of the selected neuron and the neurons located in its neighborhood(adaptation phase) - if the stop condition satisfied stop the process,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
162,Write down the SOM learning in pseudo code.,"Has three parts in it - Competition, Cooperation, Adaptation get input variable and choose amount of neurons to be more than amount of variables then run competition, where from the input neurons will be compiting to each other on choosing which fits the most after finding winning neuron change weight of neighbouring neuron only in cooperation weights of neighbouring neurons are adjusted to clusters in adaptation neurons are pulled to input variables to establish the classification",0,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
163,Write down the SOM learning in pseudo code.,- Find the winning neuron - Find the neighbors of the winning neuron.,0,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
164,Write down the SOM learning in pseudo code.,"**Pseudo code** + 1. Initialize map neurons, based on topology it could be a lattice, on a circle, etc. + 2. competition: find map neuron that is closest to an input neuron by computing distances $d$. + 3. update the position of closest map neuron with update rule. + 4. Do 2 and 3 until all input neurons are assigned a map neuron. + Do 2,3 and 4 until specified iterations or the net cumulative distance goes below some specified value or becomes zero.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
165,Write down the SOM learning in pseudo code.,Produce trainSOM; begin: randomize weights for all neurons; for (i=1 to iterationnumber) do: begin: take random input pattern; find the winning neuron; find neighbors of the winner; modify synaptic weights of these neurons; reduce learning rate and lambda; end; end;,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
166,Write down the SOM learning in pseudo code.,Initialize the network with small and random weights. Sample the data set by picking an input randomly. > Determine the winning neuron based on the output value. > Determine the coopertaing neurons based using the neighborhood function. > Update the weights of the cooperating neurons. > Adjust the learning rate. > Stop if the network converges.,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
167,Write down the SOM learning in pseudo code.,"Begin n = range of data set Initialise the weights. #We give a small random weights. for the range of n: Select a input signal, Find the winning neuron based on the similarity between the weights. Update the weights of the neighboring neuron Repeat until the convergence. 1. initalising 2. Sampling 3 Similarity matching. 4. Updating the weights 5. continuation.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
168,Write down the SOM learning in pseudo code.,intialise weights <br> while( significant change is observed in topographic pattern) {<br> take a random input (sampling) <br> find the winning output neuron (competition) <br> adjust the weights of the winning neuron and its neighbourhood neurons (cooperation) <br> continue <br> },2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
169,Write down the SOM learning in pseudo code.,"HERE: SOM learning - Initialization with random small weights. - Sampling: Picking a input pattern with certain probability. - Similarity matching: Finding the most matching neuron i.e., the winning neuron. - Synaptic updation: Updating the weights of the neuron and also the neurons in it's neighbourhood. - Continuation: Repeat steps 2 to 4 till there is no considerable change in the map.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
170,Write down the SOM learning in pseudo code.,"Parameters: $X$ data vectors, $W$ weights vectors in lattice , $\eta(n)$ -learning rate, $\sigma(n)$ - neighbourhood width, $h{ji(x)}$- neighbourhood function algo: 1) Initialize the weights to a small, random , non-repeatible values. 2) Sample a data vector with a probabiity 3) Compute the euclidean distance to weight vectors from the data points and find the winning neuron with minimum distance . 4) Update the weights of the winning neuron and its neighbourhood towards the input direction using neighbourhood function. 5) reduce the learning rate and the neighbour hood width and iterate from step 2 until no significant changes between weight vectors and inputs are seen.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
171,Write down the SOM learning in pseudo code.,1. Initialization : Initialize the weight vectors with random values such that the $wj(0)$ is different for all weights. 2. Sampling : Draw sample example $x$ from input space. 3. Similarity matching : Find the best matching weight vector for the input vector : $Wi = argmini (x - Wi(n))$ 4. Adjust the weight vectors of neurons in the neighbourhood of the winning neuron 5. Go to Sampling step and repeat until no more changes are observed in the local neighbourhood of the winning neuron.,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
172,Write down the SOM learning in pseudo code.,1. Initailization: Initialize the weights of each neuron to small random values such that weight of each neuron is different. 2. Sampling: Sample an input from the input set 3. Similarity matching: Determine the neuron nearest to the sampled input based on its distance 4. Weight updation: Update the weights of the neighbouring neurons chosen by the neighbourhood function $h{ij}(n)$ 5. Continuation: Continue from sampling until there is no more change in the weights,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
173,Write down the SOM learning in pseudo code.,"SOM is refered to as Self organized maps which is an unsupervised training algorithm for finding spatial pattern in data without using any external help.The process in SOM is explained below: -Initialization: Initialize random weights wj for input patterns - Sampling: Take nth random sample from the input (say x) - similarity matching :for the input x, find the best match in the weight vector. $i(x) = argmin(x - w)$ - update: the next step is to update the weights $w(n+1) = w(n) + eta*hji(x)*i(x)$ - continuation : continue from sampling until there is no significant change in the feature map",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
174,Write down the SOM learning in pseudo code.,"Initialization: set random small values to weights wj is different for each neuron Sampling: draw n-th sample x from input space Competition: identify winning neuron i using arg min $||x-wi||$ which means weight vector of i is most similar to input Cooperation: identify neighbors of winning neuron i using neighborhood function $h{j,i(x)} (n)$ which shrinks with time Weight adaptation: adjustments made to synaptic weights of winning neuron and its neighbors go to sampling until no large changes in the feature map.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
175,Write down the SOM learning in pseudo code.,GENERATE random weights for all neurons<br> FOR i to maxiteration DO<br> ------TAKE random input pattern<br> ------FIND the winning neuron<br> ------FIND the neighbors of the winning neuron<br> ------COMPUTE weigths of these neurons<br> ------REDUCE $\eta$ and $\lambda$<br> END FOR,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
176,Write down the SOM learning in pseudo code.,* Initialize the neuron weights randomly in a way that all neurons have different weights. * Generate random samples x from the input space. * Iterate the samples and **Compare distance between current input and all neurons** in the weight space. * **Find a winning neuron** with shortest distance from current input. * Distance is calculated using **euclidean or manhatten distance**. * Find the neurons in the neighborhood boundary of winning neuron. * **Update the weights** of neighborhood neurons using delta rule. * Adapt the size of neighborhood $(\lambda)$ and learning rate $(\eta)$ at each iteration * Repeat the process until there is no neurons in the neighborhood boundary or all the inputs moved to some neuron.,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
177,Write down the SOM learning in pseudo code.,"Step1: It selects a datapoint in random through sampling. Step2: Finds the nearest neuron through competitive learning. Step3: Updates the weight of the winner neuron and updates the weight of neighbouring neurons by a fraction. Step4: Continues steps 1, 2, 3 until there is no change in the weights or some stopping criteria is met.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
178,Write down the SOM learning in pseudo code.,"{ take a rondom point from the training data competitive phase: find the winning neuron - the neuron similar feature, using the eucledian distance formula cooperative phase: find the neighbors of the winning neuron based on the neighbor function (eg: gaussian function) adaption phase: change the weights of the all the neighboring neuron of the winning node using the formula $ \del w = \eta xj - wj $ }",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
179,Write down the SOM learning in pseudo code.,"input: distance function d(x, y), learning rate mu, neighborhood distance n Initialize the map layer with random weights for each input: find the weight which is closest to the input (minimum d(x, y)) change the weight in the direction of the input depending on the learning rate change all weights which are within the neighborhood distance n depending on their distance and the learning rate reduce learning rate and neighborhood distance",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
180,Write down the SOM learning in pseudo code.,1. Initialize small random weights. 2. Draw the nth sample from the input space. 3. Similarity matching: Determine the winning neuron. 4. Update the weights of the neuron an the topological neighborhood. 5. Repeat steps 2-4 w = random n = example.draw() wmax = getmin(wi*n) hn = getneighborhood(wmax) for wi in hn: wi = wi+$\eta$*h*y,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
181,Write down the SOM learning in pseudo code.,"Initialize the weights randomly. Create a term T1 and T2, which decrease the learning\rate and neighbourhood function respectively. Calculate $i(x) = argmin | w - x |$, the weight which is closest to the input data received. i(x) is the neuron, which wins the competetive process, this neuron and its neighbours weights are updated using $w{new} = w{old} - learning\rate \cdot h(x) \cdot (w - x)$. h(x) is the neighbourhood function which determines, which neurons are updated and how strong they are changed by the update. It is defined using the distance between the neurons. The learning\rate is updated using $learning\rate / T1$, also is the neighbourhood function updated in the same way using T2. The learning\rate cannot get lower than 0.01, while the neighbourhood function can get as low as only the winning neuron. So in the beginning almost every neuron is updated and at the end only a small neighbourhood or the neuron itself is updated.",2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
182,Write down the SOM learning in pseudo code.,for n iterations winner = competitionbetweenneurons() neighbourhood = cooperationwithneighbourhoodfunction(winner) updateweights(neighbourhood),1,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
183,Write down the SOM learning in pseudo code.,given a map layer set random small values for weights from input to map layer repeat until not converged: find best match of input value and weight of the neurons (competitive process) adapt (increase) weight of winning neuron and neighboorhood (with gauss function and neighboorhood size) (cooperating process and weight adjustment) decrease neighboorhood size,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
184,Write down the SOM learning in pseudo code.,,0,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
185,Write down the SOM learning in pseudo code.,Initialize weight vectors of hidden neurons with same dimmension as of data. Number of hidden neuron should be signifiacntly greater than number of data points. initialize learning rate n and neghbouring function h while (rate of change in weights is significant): for every datapoint: calculate distance of each neuron from data. select winner neuron w with minimum distance (maximum similarity) error = distnce of winner form datapoint adjust weights of neurons with the rule w = w + n*h*error,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
186,Write down the SOM learning in pseudo code.,randomly inilize the weights draw sample of inputs Increase the weights of the local neihburhood of winning neuron repeat the process above process till there is only one winning neuron,1,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
187,Write down the SOM learning in pseudo code.,,0,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
188,Write down the SOM learning in pseudo code.,for i in numofepochs for p in inputpoints find the winning neuron find the neighbours of the winning neuron within distance sigma update winning neuron and neighbours weight update sigma and learningrate so that both reduces over time,2,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
189,Write down the SOM learning in pseudo code.,,0,"Arrange the weights in the required topology according to the problem. Initialize the weights randomly such that all the weights are different. Sample the input from the input space. Similarity matching: match the input to a neuron in the topological lattice which becomes the winning neuron. Update the weights of the winning neuron and its neighbours determined by the neighbourhood function. Reduce the neighbourhood and decay the learning rate and share radius. If ordering and convergence are complete, stop. Else continue sampling from the input space."
190,Give the basic idea of an SVM using the correct terminology!,"A support vector machine is a maximum margin classifier in which the width of the boundary of separation is maximized. A margin is defined as the width of the boundary before hitting a point. This maximum margin intuitively feels safe, and is experimentally good.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
191,Give the basic idea of an SVM using the correct terminology!,"Basic idea of SVM is to best segregate the data into two classes with the help of decision boundary. This decision boundary is margin, we always try to maximize the margin to make sure data is classified correctly",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
192,Give the basic idea of an SVM using the correct terminology!,Support Vector Machines goal is to maximize margin between closest data points of separating hyperplane. Separating hyperplane is given by: 0 = w(n)*x(n) + b. By maximizing margin probability of classification errors is reduced.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
193,Give the basic idea of an SVM using the correct terminology!,"An SVM is a binary, linear classifier spanning a seperating hyperplane between two classes of datapoints. The hyperplane is spanned between both the positive and negative decision boundaries, and supported by a number of support vectors. Support vectors are the outermost datapoints which span the hyperplane. During training, the distance of falsely classified data points to their correct side of the hyperplane is minimized, utilizing a quadratic programming formulation.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
194,Give the basic idea of an SVM using the correct terminology!,A SVM is a binary classifier with a maximum width boundary separating the two classes. This uses support vectors (vectors that pushes against the boundaries). The equations of the lines in an SVM are: - $wx+b>=1$:for class 1 - $wx+b<=-1$:for class -1 - M is the width between these boundaries.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
195,Give the basic idea of an SVM using the correct terminology!,Because SVMs are binary classifiers we can use a border to sperate the data. The border is typically placed where it has the largest possible distance to both classes. The vectors the border touches on both sides with its margin are the support vectors.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
196,Give the basic idea of an SVM using the correct terminology!,"A SVM is an ANN for supervised learning, whicht is able to saperate two classes of data-points by using a hyperlane found by quadratic programming, by finding the biggest margin. The goal is to classify future data in there two classes.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
197,Give the basic idea of an SVM using the correct terminology!,The SVM is a maximum margin classifier. It is used the binary classify datapoints in a dichotomy. The idea is to find a line wich linearly seperates both classes. There perfect position of this line is in right in the middle of these classes. To find this line(descision boundary) we define a positive and a negative boundary which are parallel to this line. The boundarys define the margin between both classes. The idea of SVM is that the datapoints which are next to the boundary can be used to define the margin. They are called support vectors. Addittionaly not every problem is linearly seperable so the idea was to transform the input into many higher dimensions using some kernel functions. We discussed the kernel function of polynomial terms and found out that it easy to compute.,2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
198,Give the basic idea of an SVM using the correct terminology!,"Support Vector Machines are a type of learning machines that try to classify different classes of an input space. For linear separable classes, the SVMs try to calculate the line that separates this two classes with maximum margin. The support vectors will be the points closer to this margin. When the input data is noisy, we have an optimization problem of two aspects (maximum margin, proper classification). So, a trade-off (C) will be defined. The trade-off will be calculated by the sum of the distance of misclassified points. For non-linear separable classes, a kernel will be defined that will transform the input data into a higher dimensional space.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
199,Give the basic idea of an SVM using the correct terminology!,In linear SVM we have a linear border line classifier that seperate two different classes(positive and negative planes) and we calculate the distance of the data points from this border line classifier. Also a margin will be defined and this margin will be maximized until it touches some data points in the plane. The data points that the margin pushed agains them will be our support vectors. The error for the wrongly classified datapoints will be calculated by calculating the distance of the data point from its correct plane. The SVM tries to learn the classifier and the margin from the training data.,2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
200,Give the basic idea of an SVM using the correct terminology!,"Support vector machines are classifiers that are using support vectors, which are variables of the dataset. These variable are chosen during learning algorithm. Main advantage of SVMs is that it will not be overfitting by choosing correct margin. Activation functions can be both linear and nonlinear. Output of SVM is always TRUE or FALSE for given variable.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
201,Give the basic idea of an SVM using the correct terminology!,Support vector machines are a type of neural network that build a desicion boundary around classes such that the margin of separation between classes is maximized.,0,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
202,Give the basic idea of an SVM using the correct terminology!,SVMs are binary classifier. They learn the classification by memorizing the marginal data points (called support vectors) that make up the decision boundaries (2 : positive and negative).,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
203,Give the basic idea of an SVM using the correct terminology!,"The abbreviation SVM stands for Suppor Vector Machine. SVMs represent a feedforward category of NN. SVMs are binary learning machines whose functionality can be summarized for classification problem as follows: Given a training sample, the SVM constructs a hyperplane as the decision surface in such a way that the margin of seperation between positive and negative examples is maximized. One key innovation associated with SVMs is the kernel trick. The kernel trick consists of observing that many machine learning algorithms can be written exclusively in terms of dot products between examples. It allows us to learn models that are nonlinear as a function of x using convex optimization techniques that are guaranteed to converge efficiently. Besides, the kernel function k often admits an implementation that is significantly more computatinal efficient than naively constructing two vectors and explicetly taking their dot product.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
204,Give the basic idea of an SVM using the correct terminology!,"An SVM, or support vector machine, is a feedforward network with a hidden layer to learn a task in a supervised learning manner. The network tries to construct a hyperplane that separates the data points of two different classes by maximizing the margin of separation, which is the distance from the hyperplane to the closest data points called support vectors.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
205,Give the basic idea of an SVM using the correct terminology!,"Given a dataset, support vector machines builds a hyperplane in a such a way that positive and negative samples are seperated to the maximum distance. Width of the margin should be maximum The vectors to which the margins(margin for positive and negative sample) are pushed on to it are called support vectors.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
206,Give the basic idea of an SVM using the correct terminology!,SVM stands for Support Vector Machine. It creates a hyperplane such that margin of separation between positive and negative classes is maximised.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
207,Give the basic idea of an SVM using the correct terminology!,HERE: SVM is a linear machine whose goal is to construct a optimal hyperplane such that the marginal separation is the maximum between the decision boundaries. The decision boundaries are drawn parallel to the hyperplane which just push the datapoints closest to the hyperplane. The datapoints closer to the hyperplane are called support vectors.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
208,Give the basic idea of an SVM using the correct terminology!,The idea of SVM is to fit a supervised model onto the training data allowing maximum generalization ability. This is done by computing maximum margin between different classes of data using the support vectors. The magrin can be computed using differeent kernels for a higher dimensional data.,2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
209,Give the basic idea of an SVM using the correct terminology!,A SVM is a linear machine which is used in pattern classifcation problems to find a decision surface in the form of a hyperplane for linearly separable classes such that the margin of separation between the classes is as large as possible. SVM's are an approximate implementation of the induction principle of structural risk minimization which is based on the fact that the error rate in testing is bounded by a term that is dependent upon the sum of training error rate and the VC dimension of h.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
210,Give the basic idea of an SVM using the correct terminology!,The basic idea of SVM is to determine the best decision boundary i.e. the one which provides maximum margin so that the boundary can be widened most before it touches any datapoint. It is done using Support Vectors which are the the datapoints the margin pushes against.,2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
211,Give the basic idea of an SVM using the correct terminology!,"SVM refers to support vector machines.In terms of a linear classification problem svm can be defined as creating a hyper plane which is a decision surface and to maximize the width of decision boundary.In cases where the problem is complex svm can be used as it classifies the data by projecting the data in higher dimension.If the data is to be separated in 3 classes , they can use 3 svm's for three different classes.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
212,Give the basic idea of an SVM using the correct terminology!,basic idea of SVM is to construct a hyperplane as the decision surface in such a way that the margin of separation between negative examples and positive examples is maximized.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
213,Give the basic idea of an SVM using the correct terminology!,The idea of SVM is to construct a hyperplane as a decision surface such that the margin separation between positive and negative examples is maximized.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
214,Give the basic idea of an SVM using the correct terminology!,SVM tries to find a **best hyperplane with widest margin with the help of support vectors** such that all the data points are classified correctly.,2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
215,Give the basic idea of an SVM using the correct terminology!,"SVM is used for linearly separable data. A hyperplane is used to separate the data, but there could be so many hyperplanes that separate the data. The best hyperplane is choosen which separates data with a bigger margin. So in SVM we find the hyperplane which has a bigger margin between the hyperplane and both the positive and negative data lines.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
216,Give the basic idea of an SVM using the correct terminology!,"Given a training set for classification, The basic idea of SVM is to construct a hyperplane as decision boundary such a way that the margin between the positive and negative points is maximum Support vector is a small subset of the of the training data against which the boundary is pushed",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
217,Give the basic idea of an SVM using the correct terminology!,"A support vector machine classifies given data using a decision boundary. The width of this decision boundary (margin) is maximized to ensure good results, because a maximized width is as robust as possible. The margin width is $\frac{2}{\sqrt{w * w}}$. To maximize it, quadratic programming is used. In order to handle noisy data, slack variables are introduced. To eliminate them, duality is used.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
218,Give the basic idea of an SVM using the correct terminology!,"An SVM is a linear classifier that divides a binary pattern, by a line that maximizes the margin between its line and the respective support vectors.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
219,Give the basic idea of an SVM using the correct terminology!,"A SVM learns a decision boundary from the input data. Additionaly it learns two margins, which are parallel to the decision boundary and lie as close as possible at the data points, the support vectors. The decision boundary is chosen so that the margins are maximized. Using kernel functions higher dimensional data and non linearly separable data can be learned aswell.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
220,Give the basic idea of an SVM using the correct terminology!,"An SVM is a learning machine that tries to learn the support vectors of a two class data set to get the maximum margin, the optimal seperating hyperplane, between the two classes.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
221,Give the basic idea of an SVM using the correct terminology!,"A SVM uses a few of the data points as support vectors to bild the maximum margin classifier. It searches for the seperating line, which has the maximum margin to the datapoints. In cases of Noise, the seperating line is searched, which minimizes the distance to the points in the wrong category. The data is cast to a higher dimensional space to use covers theorem while using kernels. The data is more likely linearly seperable in the higher dimensional feature space. Using structural risk minimization the dimensionality is reduced.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
222,Give the basic idea of an SVM using the correct terminology!,"SVMs are used to linearly seperate Data points. The decision boundary is line or hyperplane in higher dimensions that defines the lable of a data point. The decion boundary is choosen in a way that the margin is maximized. Data points on the decion boundary are called support vectors and define the hyperplane. In 2 dimensions if the data is liner seperable the margin is equal to 2/sqrt(w.w) where w is the weight vector. If the data is not linear seperable, the input can be projected into higher dimension space. This increases the chance of linear seperablity.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
223,Give the basic idea of an SVM using the correct terminology!,Support vector machine is classifier which maximizes the margin between boundries learned from two classes. Margin is minimum distance by boundries can be increased before hitting datapoints. Support vectors are the datapoints against which magin pushes up the boundary.,2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
224,Give the basic idea of an SVM using the correct terminology!,"support vector machines are the finding classfiers, draw the dision boundary which push against the support vectors.",1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
225,Give the basic idea of an SVM using the correct terminology!,The basic idea of Support Vector Machine (SVM) is to find the width of a line or hyperplane which which divides the input data into two classes. The points lying on the edge of the defined width are called support vectors.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
226,Give the basic idea of an SVM using the correct terminology!,SVM is a classifier that classifies a set of points in a way that maximizes the margin between the points of two classes. The classification can be linear or non linear.,1,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
227,Give the basic idea of an SVM using the correct terminology!,"The idea behing SVM is to find a hyperplane which separate data into classes. First it is required to find a data point which are clossest to hyperplane, and these data points are called support vectors. Next task is to find a maximum possible width of the hyperplain such that support vectors are on the edge of that hyperplane. This problem is formulated as min-max constrained optimization problem. In order to find a optimum width of hyperplane, (optimimum of a funtction) the idea is to use method of Langrange multiplier. Additionally, when I data is not linearly separable, than an approach is to project data in higher dimension and then to find a hyperplane that separates data in that dimension.",2,SVMs are linear learnable machines in the simplest case. It uses a decision boundary with maximum margin to classify the data into different classes. The data points which are near the decision boundary are called support vectors and the margin is determined based on these points. Kernels are used to separate non-linearly separable data and the algorithm is solved by using Quadratic Programming. 
228, What role does the method of steepest decent have when learning a network?,"In steepest descent, the gradient of the cost function is found by partially differentiating it with respect to the weights. The weights are then updated in the opposite direction if the gradient. This ensures that the weight moves in the steepest direction are reduced. It can also be proven that the weights always reduce. Hence, steepest descent can be used to minimize the cost function.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
229, What role does the method of steepest decent have when learning a network?,"Steepest descent is method of optimizing the algorithm by minimizing the error. Weights are adjusted in the direction of steeping descent, opposite to the direction of the gradient.",1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
230, What role does the method of steepest decent have when learning a network?,Steepest descent moves the error within error surface a small step into the opposite direction of gradient. By help of steepest descent we want to minimize error. Steepest descent stops when gradient = 0.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
231, What role does the method of steepest decent have when learning a network?,"When learning weights with a SD method, we try to reduce the error based on following the gradient of an error function in the opposite direction, effectively trailing the error surface towards the minimum. Here, the error function (typically some form of mean squared error) is differentiated w.r.t. the individual weights, expressing how much a weight contributes to the network error and must thus be corrected. Due to the gradient pointing in the direction of steepest ascent, we must thus step in the negative direction.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
232, What role does the method of steepest decent have when learning a network?,"- Steepeset descent is used for error minimization when updating weights. - According to this, we update the weights along a direction which minimizes the error; which is calculated by finiding the slope at the point.",1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
233, What role does the method of steepest decent have when learning a network?,Speepest decent is used to minimize the training error of a network given sample inputs and desired outputs. It uses the gradient of the error function to move the weights closer to an optimal weight with lowest output error. Using a learning rate we can influence the speed and stability of this algorithm.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
234, What role does the method of steepest decent have when learning a network?,The steepest decent is the direction the error function falls the most. We want to change the weights in the direction of the steepest decent (the opposide direction of the gradient) to have a smaller error in the next iteration and to optimize the ANN.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
235, What role does the method of steepest decent have when learning a network?,"The idea of learning a network is to minimize a certain costfunction. We can use steepest descent to minimize this cost function. While there are other optimization techniques which can be used for optimization, steepest decent is a widly used optimization technique. To optimize a network we calulate the partial derivatives(gradient) and use it to update our weights. It is also used in BP.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
236, What role does the method of steepest decent have when learning a network?,"The approach of the method of steepest descent is to find the direction for the minimization of the error in an approximation problem. The cost function e, dependent of the weights w, will be derivated (partial derivative) for all defined weights. This gradient will be used for updating the weights for the next iteration. The direction of the minimization of the error is the oposite direction of the gradien: - g.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
237, What role does the method of steepest decent have when learning a network?,When the inputs are being send into a network and we calculate the error we need a mechanism to learn and manipopulate the free parameters of the network and the learning uses the error but we must know in which direction in the search(optimization) space we should move so that we can reach the global minima of the error for this we use steepest decent. This method tells us in which direction we need to move by getting the gradient from the error.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
238, What role does the method of steepest decent have when learning a network?,Steepest descent is a method of weight adaptation. It is using first order derivative to approximate the function. Therefore is rather slow.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
239, What role does the method of steepest decent have when learning a network?,The steepest descent is an unconstrained optimization method that seeks to minimize an error function. This function is iteratively changed in direction oposite to the gradient vector.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
240, What role does the method of steepest decent have when learning a network?,Method of steepest descent updates the weights in the direction where the error is minimum.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
241, What role does the method of steepest decent have when learning a network?,The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. This property is used to determine the optimal weights of the NN.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
242, What role does the method of steepest decent have when learning a network?,Steepest descent is used to update the synaptic weights of a network based on a cost function expressed by the errors of the output. The weights are adjusted in the direction opposite to the gradient of the cost function.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
243, What role does the method of steepest decent have when learning a network?,"In steepest descent the adjustments done on the weight vector are in the direction of the steepest descent which is in the direction opposite to that of a gradient descent. In a learning problem, it basically used to reduce the cost based on the weight. The main goal is to find an optimal weight.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
244, What role does the method of steepest decent have when learning a network?,Method of steepest decent is an unconstrained optimization technique used for learning in a network. It is used in iterative manner to minimize the error in supervised learning. It finds the direction of maximum gradient. So we go in the opposite direction hoping to find the minima. Convergence of the algorithm depends on the learning rate and also the condition that it doesn't get stuck in local minima.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
245, What role does the method of steepest decent have when learning a network?,"HERE: In steepest descent method, the network moves towards the direction of the maximum gradient. The learning with steepest descent method can be slow to converge and can exhibit zigzag behavior.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
246, What role does the method of steepest decent have when learning a network?,Steepest descent involves weight updation in the direction of maximum steep or maximum derease in the cost function ot in the direction opposite to the gradient funcion. The weight update is $\Delta w(n) = - \eta g(n)$ where $\eta$ is the learning rate which defines the magnitude of learning using the gradient g(n) which is the gradient of the cost function of errorsin the nth iteration. Higher $\eta$ will result in rapid learning but with oscilations in responses.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
247, What role does the method of steepest decent have when learning a network?,"The method of steepest descent is used to find the direction in which the error function viewed as a function of weights is decreasing most rapidly and then take a small step in that direction. When learning a network, steepest descent enables to iteratively adjust the weight vectors until the optimal weight vector that minimises the cost function (i.e, the error function where error is computed as the difference between the desired and actual response of the network) is found.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
248, What role does the method of steepest decent have when learning a network?,When learning a network the steepest descent algorithm updates the weights in such a way that the error decreases in every iteration.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
249, What role does the method of steepest decent have when learning a network?,The method of steepest descent moves in the direction opposite to the gradient to minimize the cost funcion .,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
250, What role does the method of steepest decent have when learning a network?,"steepest decent method is based on minimization of error cost function $\xi(w) = 0.5 e^2k(n)$, so synaptic weight of network is updated in a direction opposite to gradient vector of $\xi(w)$, that is $Wk(n+1) = Wk(n) - \eta \nabla \xi(w) = Wk(n) - \eta ek(n) x(n)$, $\eta $is learning rate.$ek(n)$ is neuron k error signal, $xj(n)$ is input data.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
251, What role does the method of steepest decent have when learning a network?,The steepest is used to find a direction in which E is decreasing most rapidly. The adjustments applied to the weights are in the direction of steepest descent.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
252, What role does the method of steepest decent have when learning a network?,Steepest decent helps to **minimize the value of error function $E$** by finding the **right direction **to move the weight vector to reach global minima. The direction is always **opposite to the direction of actual gradient vector**.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
253, What role does the method of steepest decent have when learning a network?,"Method of steepest descent is used to reduce the error. In backpropogation during backward pass we need to know how by how much amount the weights should be changed, this can be known if we use steepest descent, find the gradient of error and use it to reduce the error.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
254, What role does the method of steepest decent have when learning a network?,The steepest descent finds the direction of the error function and tries to reduce it by adding in the opposite direction $ del w = - \eta g(n)$ g(n)- gradient of the cost function,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
255, What role does the method of steepest decent have when learning a network?,The steepest descent is used to find the right direction in which the weights should be changed while learning a network. The derivate of the error is used and weights are changed in that direction which makes the error smaller as fast as possible.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
256, What role does the method of steepest decent have when learning a network?,The steepest descent can be used to optimize the weights of a network. In steepest descent the error function is a function of the weights. So we determine the direction of the steepest descent on the error surface and go into that direction to minimize the error of the weights on optimize them.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
257, What role does the method of steepest decent have when learning a network?,"The method of steepest descent is used to minimize the error function. The error function is the gradient of the error $\Delta e = d - y$, where d is the desired output and y is the actual output of the neuron.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
258, What role does the method of steepest decent have when learning a network?,"Steepest descent is the basic learning algorithm others are derived from. The goal when learning a network is to minimize the error. This is achieved by starting at a random position and going in the opposite direction of the gradient vector, the steepest descent.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
259, What role does the method of steepest decent have when learning a network?,the error function is computet. to adapt the weights (learn the network) the error function is followed in small steps in direction of steepest descent to decrease the error. using iterations the error is decreased in each step and end in a (local) minimum used in back-propagation,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
260, What role does the method of steepest decent have when learning a network?,"In error correction learning the weights of a network are learned in a way that e(x) is minimized, where e(x) is some error function. In order to minimize the error function the method of steepest desend is used. The negative gradient of e(x) points in the direction of steepest decend. Doing steepest descend in a single layer feed forward network leads to the delta rule.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
261, What role does the method of steepest decent have when learning a network?,Steepest descent adjust the parameters (weights and bias) of the NN to minimize the error. It does so by adjustinmg the weights in the direction of steepest descent of the error function.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
262, What role does the method of steepest decent have when learning a network?,Steepest decent while move in the direction of the max improvement ( in terms of decrasing) in the cost funtion or error. if the learning rate is large then the it follows the zizag motion. if the learning rate is too low then it takes time for converging .,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
263, What role does the method of steepest decent have when learning a network?,The method of steepest descent is responsible for weight adjustments in the network. The weights are adjusted in the direction of the steepest descent that is equal to the negative grad of the error. It ensures that the weights are decreased in every iteration step.,2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
264, What role does the method of steepest decent have when learning a network?,Steepest decent method helps in making the adjustments of the weights in a neural network in a way that minimizes the average squared error. In each step it gives the direction towards which the maximum decrease of the average squared error can be achieved.,1,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
265, What role does the method of steepest decent have when learning a network?,"The method of steepest decent is used for finding minimum of a cost(error) function. The steepest decent iterates over possible values of weight vector to optimize the function. It is used for deriving error function in ADALINE (adaptive linear element) algorithm, and it is used also in backpropagation method in training of Multi-layer NNs.",2,Steepest descent is used to update the weights in a NN during the learning phase. It helps to navigate the cost function and find the parameters for which the cost is minimum. The weights are updated in the direction of the steepest descent which is in a direction opposite to the gradient vector. This method could suffer from local minima and may become unstable.
266,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"A dataset $A \subseteq X $ with N datapoints has $2^N$ binary maps. If for any of these binary maps, a hypothesis $h \in H$ splits the positive data from the negative data such that there is no training error, then it is said that h shatters the dataset A.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
267,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,,0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
268,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,A a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$ if there exists a an $\alpha for every training set with zero training error,1,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
269,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"... when our learned machine achieves zero training error on every classification problem of the dataset A. Since we got a selection of $n$ points in the dataset A, the number of problems in binary classification is 2 to the power of $n$ (I didnt find the 'Dach' symbol on the english keyboard :) )",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
270,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"Considereing a dataset $A \subseteq X$ ,where X is the instance space and A contains N elements. Now there are $2^N$ binary maps or learning problems when we wnat to separate two classes. If any of these problems can be separated completely by hypothesis $h \in H$ then h is said to shatter A. i.e., a hypothesis shatters a dataset, if it can completely separate the classes with zero error for all possible combination of labels in the dataset.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
271,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,when every possible combination of input and desired output can be classified using $h$,1,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
272,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"A hypothesis $h \in H$ shatters a dataset $A \subseteq X$, then for every point $xi \in A$ there is a label $yi \in \{1,-1\}$ and the $H$ can saperate these two classes using $h$ with no training error.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
273,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,there exists an arrangement of these points in A sucht that for each possible combination of labels to these points the hypothesis h has zero training error,0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
274,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"An hypthesis *h* shatters a dataset A, if for a given data set, h is able to distinguish (or separate) the different classes of this data set.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
275,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"""h"" shatters A if for any set of input data points in A there exist at least one training error of zero.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
276,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"H shatters A when for example in given dataset (X1,X2...Xr) output are in a form (X1, Y1),(X2,Y2)...(Xr,Yr) there has been found a 0 error.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
277,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"A machine F can shatter a set of points $x1, x2, x3,..., xn$ if and only if for every training set, there is a weight vector $\alpha$ that produces zero training error.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
278,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"A hypothesis $h \in H$ shatters a dataset $A \subset X \Leftrightarrow$ for each assignable configuration of $(xi, yi)\in A$, $h$ perfectly classifies all elements of the set $A$.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
279,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,,0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
280,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,A hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow$ at least on possible combination of dataset $A$ can be classified by the hypothesis $h \in H$ with zero training error.,0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
281,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"Given a dataset $A \subseteq X $ where X is the instent data space, for a given problem with the dataset A, if a learning machine is able to successfully split the positive and the negative data, then we say that A is shattered by the learning machine.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
282,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,Suppose X is a training dataset and A is the subset of training dataset then hypothesis h is said to shatter if can correctly classify all the points in A i.e zero training error.,2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
283,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,HERE: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$ if the hypothesis can clearly distinguish the positive examples from the negative examples in A.,1,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
284,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"A hypothesis h is model that separates a dataset consisting of {(xi , yi)} samples into positive and negative samples. h is said to shatter a given subset of a dataset if it can successfully separate at least one configuration of the subset of dataset.",0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
285,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,A hypothesis $h \in H$ shatters a dataset $A \subseteq X $ if there exists an $\alpha$ for which there is zero training error,2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
286,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"for each of the $2^N$ (where N is the size of A) combinations of input output mappings of the form $(Xi, yi)$, h is able to classify the data correctly that is with zero error.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
287,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,,0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
288,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"For all possible binary labeling of dataset A, we can find a hypothesis h that can separate the positive examples from negative examples, the H shatters A.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
289,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"The hypothesis $h$ can shatter any points of $x1$, $x2$, ..., $xn$ if and only if for every possible training set of the form $(x1, y1), (x2, y2), ... (xn, yn)$ there exist some values of $\alpha$ that gets zero training error.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
290,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"A hypothesis space H shatters a dataset, if and only if there is a **possbile $\alpha$ (weight vector)** on hypothesis space that **seperates all the positvie data from negative data**.",1,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
291,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,a hypothesis $h \in H$ shatters $A \subseteq X$ if and only if there exists a value of $\alpha$ for which the training error is zero,1,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
292,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"H is the vc dimension of a learning maching that can shatter h points. Vc dimension of a learning machine is the maximum number of points that can be arranged so that the learning machine can shatter them Shattering: The learning machine is said to shatter points $(x1 ... xr)$ if and only if all the possible training set of $((x1,y1) ... (xr,yr))$ can be classified with zero training error",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
293,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,A hypothesis shatters a dataset if it can correctly classify all combinations of labellings of the points in the dataset.,2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
294,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,", if there exists a configuration of $X$, so that $h$ gets zero training error on any dichotomy of the datapoints.",0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
295,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"there exist w weights, which produce a perfect classification.",0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
296,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,for all possible classified subsets of dataset A the hypothesis h can seperate it,2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
297,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,when all combinations of position and labeling of the data can be separated in the given classes by the hypothesis,2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
298,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"h shatters A when and only when for all possibilities of (a1, y1), (a2, y2), ... ,(an, yn), where y is the class lable (1 or -1) there exists some $ alpha $ for a learning machine f that produces 0 training error.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
299,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,If there exist atleast one configuration of A for which training error of h is zero. i.e. it successfully classifies all oints in A.,0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
300,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,and there exist a linear saperater which saperates positve examples from the negavtive examples correctly. then we say that A can be shatter at h.,2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
301,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"Ginven a data set A if it is possible to find a hypotheis H which separates the data set into binary form without any error, we can say that hypothesis $h \in H$ shatters dataaet $A \subseteq X \Leftrightarrow \ldots$.",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
302,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"It means that for all the points in A with input output pair (x,y), for any combination of ($xi$,$yi$) there exist parameter $\alpha$ of h that enables h to classify the points with zero error",2,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
303,Define: a hypothesis $h \in H$ shatters a dataset $A \subseteq X \Leftrightarrow \ldots$,"We say that a hypothesis h shatters a dataset A, iff the h produces a zero training error for certain data set A. In other words, we say that a hypothesis h shatters a dataset A, when h separates data A in two classes without erorr.",0,"For all 2^N possible binary labeling of every data, if a hypothesis h splits the positive data from the negative data with no error, then it means that the hypothesis h shatters the dataset A."
304,Write down and explain the Widrow-Hoff learning rule!,"$ \Delta w = \eta e(n)x(n) $, where $\eta$ is the learning rate. Widrow-Hoff rule states that the change in weights is proportional to the product of the error and the input in the corresponding synapse.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
305,Write down and explain the Widrow-Hoff learning rule!,"Weights adjusted are proportional to the product of error signal and the input vector w(n + 1) = w(n) + $\eta(d-y)x(n)$ $\eta$ is learning rate, d is desired output, y is current output. x(n) in input vector.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
306,Write down and explain the Widrow-Hoff learning rule!,Adaption of weight is proportional to product of input and error: $w{new} = w{old} + x*e$,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
307,Write down and explain the Widrow-Hoff learning rule!,"For neurons with a linear activation function (ADALINE): $w(t+1)=w(t)+\alpha (d-y)x$, where x is the input pattern, d is the true value and y is the net output. Notice that the delta rule looks similiar to the perceptron learning rule, but was derived from SD, whereas the perceptron works with a step function which is not fully differentiable.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
308,Write down and explain the Widrow-Hoff learning rule!,"Widrow-Hoff learning rule is also known as error correction rule is used to update the weights as: $\Delta w = \eta (di-yi)xi$ where, d is the desired output and y is the output the network generates and x is the input.",1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
309,Write down and explain the Widrow-Hoff learning rule!,$w(n+1) = w(n) + \mu (d(n) - y(n))x(n)$ The change of the weights is determined using the error ($d(n) - y(n)$) and the input that was given to the network. The learning rate can improve learing speed. The new weights are dependent on the old ones and the change calculated,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
310,Write down and explain the Widrow-Hoff learning rule!,$w{ij}(n)= w{ij}(n-1)+ learningrate*(dj-yj)*xi$ we change the weights by computing the error $ej= (dj-yj)$ for the input and multiply it by the learningrate and the $xi$ and adding it to the old weight. This minimises the squared error function (our cost function) and is the online variant of the steepest decent method.,2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
311,Write down and explain the Widrow-Hoff learning rule!,$$ \Delta w(n) = \eta * e(n)*w(n) $$ $$ e(n) = (y-d) $$ The widrow hoff learning rule is error correction learning. It is used to train a network in a supervised manner. The widrow hoff learning rule can be derived from gradient decent. The rule consists of the error e(n) the neuron has and is muliplied with the weight so that the impact of the weight to the error is incorporated into the update. A learning rule is use as a adjustment in how much we trust the weight change. The error is calculate by the difference between the current and expected output.,2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
312,Write down and explain the Widrow-Hoff learning rule!,"The Widrow-Hoff learning rule is defined as: $w(n + 1) = w(n) + \eta * x(n) * e(n)$ The Widrow-Hoff learning rule is a rule for adjusting the weights of a NN for a error correction learning task. This learning rule is derived from the steepest descent method, where the direction for the minimization of the error is the defined as the oposite direction of the cost function's gradient. This gradient can be simplified as $x(n) * e(n)$, where e(n) is defined as the difference between the desired response and the actual response of the learning machine (NN): $e(n) = d(n) - y(n)$. $\eta$ defines the learning rate used.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
313,Write down and explain the Widrow-Hoff learning rule!,,0,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
314,Write down and explain the Widrow-Hoff learning rule!,"Windrow-Hoff rule is $$W{new}=x{input}*W{old}*(d{output}-y{output})*eta*a $$ where $W{new}=new weight,W{old}=old weight,d{output}=desired output,y{output}=actual output,x{input}=input, eta=learning rate, a=learning constant$",0,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
315,Write down and explain the Widrow-Hoff learning rule!,The Widrow-Holf or delta rule is a gradient descent learning rule used to adapt weight in a perceptron. $\Delta w(n) = - \eta(d(n) - y(n))x(n) $ $\Delta w(n) = - \eta e(n)x(n) $,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
316,Write down and explain the Widrow-Hoff learning rule!,"The widrow-Hoff (delta) learning rule is given by $$ w(n+1) = w(n) - \eta x(n) e(n)$$ where $e(n)$ is the error vector, $\eta$ is learning parameter, $x(n)$ is input vector.",1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
317,Write down and explain the Widrow-Hoff learning rule!,"The Widrow-Hoff Learning rule is also referred to as Delta, or Least Mean Square (LMS) Rule. It is used to minimize the cost function and is defined as follows: Delta wji(n) = eta (partial xi(n) / (partial wji(n)) where eta is the learning rate paramter, xi(n) is the total instantaneous error energy and w are the weights.",1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
318,Write down and explain the Widrow-Hoff learning rule!,"The Widrow-Hoff learning rule, also called delta rule, is used for learning a network by adjusting the synaptic weights of the network with the error signals: $$ w(n+1) = w(n) + \eta (d(n) - y(n)) x(n) $$ where $n$ is the number of iteration, $\eta$ is the learning rate, $d(n)$ is the desired output signal, $y(n)$ is the actual output signal, and $x(n)$ is the input signal. $(d(n) - y(n))$ is the error signal.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
319,Write down and explain the Widrow-Hoff learning rule!,Widrow hoff's learning rule states that the adjustment of the weight of a synapses are propotional to the product of the error function and the input which is given by the synapses based on the problem.,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
320,Write down and explain the Widrow-Hoff learning rule!,Widrow Hoff rule is based minimising the mean square error using gradient descent alogirthm. Weights are adjusted in following manner:<br> w(n+1) = w(n) - n (gradient of mean square error) <br> It takes the gradient of the mean square error $0.5 e^{2}(n) = e(n) \frac{\partial e(n)}{\partial w} = e(n) x(n)$,2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
321,Write down and explain the Widrow-Hoff learning rule!,"HERE: Widrow- Hoff rule: - $\Delta w$ = $\eta e(n) x(n)$ - Widrow-Hoff rule states that when an input x(n) produces an error e(n), then the change in the weight is directly proportional to the error signal and the input signal.",1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
322,Write down and explain the Widrow-Hoff learning rule!,"Widrow Hoff learning rule is also called as error corresction learning rule. The error is defined as the difference between the desired and the actua output of the learning machine. Assuming the desired signa is available, the error is computed and weights of the neural network are upadted in the direction of reduction of errors. The error for each input sample for a neuron k is computed using $ek(i) = dk(i) - yk(i)$. weight change $\Delta W = W*e$, that is the dot product of error and the weights is computed.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
323,Write down and explain the Widrow-Hoff learning rule!,"Given a neuron k excited by an input signal $xi$, if $w{ki}$ is the synaptic weight of the neuron, then the Widrow-Hoff learning rule gives the weight adjustment $\Delta w{ki}$ applied to the neuron k in mathematical terms as follows: $\Delta w{ki} = \eta xi(n)e(n)$ where e(n) is the instantaneous value of the error signal. Thus the Widrow-Hoff rule states that the synaptic adjustment applied to the weights of a neuron is proportional to the product of the input signal to the neuro and the instantaneous value of the error signal. This rule assumes that the neuron has an external supply of desired response so that the error can be computed.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
324,Write down and explain the Widrow-Hoff learning rule!,The Widrow-Hoff learning rule is given by $$w(n + 1) = w(n) + \eta e(n) x(n)$$ where $w(n)$: Weight in iteration n $e(n) = d(n) - y(n)$: Error $d(n)$: Desired output $y(n)$: Actual output $x(n$: Input $\eta$: Learning rate,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
325,Write down and explain the Widrow-Hoff learning rule!,"Widrow -Hoff learning rule states that the adaptation made to the synaptic weights is proportional to the product of input and the error function.It basically states that if the error is high then the product of input and error will also be high , and thus the adjustment made to the weight would be more. $wj(n+1) = wj(n) + eta*(error)*input$",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
326,Write down and explain the Widrow-Hoff learning rule!,"it is based on minimization of error cost function $\xi(w) = 0.5 e^2k(n)$, so synaptic weight from neuron k to input j is updated in a direction opposite to gradient vector of $\xi(w)$, that is $w{kj}(n+1) = w{kj}(n) - \eta \nabla \xi(w) = w{kj}(n) - \eta ek(n) xj(n)$, $\eta $ is learning rate.$ek(n)$ is neuron k error signal, $xj(n)$ is input data.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
327,Write down and explain the Widrow-Hoff learning rule!,Windrow-Hoff or error correction learning rule says that the adjustment of a weight is proportional to the product of the error signal and the input signal of the weight.,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
328,Write down and explain the Widrow-Hoff learning rule!,"$$\bigtriangleup \omega{ji} = ej * xi$$ $$\omega(n+1) = \omega(n) + \eta \bigtriangleup \omega{ji}$$ Widrow Hoff learning rule says that, the synaptic weight update is directly proportional to the product of error and the input.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
329,Write down and explain the Widrow-Hoff learning rule!,Widrow-Hoff learning rule: The rules states that the weight update is directly proportional to the product of the input to the neuron and the error. $\Delta w{ij} = \eta e(n) \sum xi(n)$,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
330,Write down and explain the Widrow-Hoff learning rule!,delta $ w{kj} = \eta ek . xj $ Widrow hoff rules states that the change in synaptic weight is proportional to the product of the error signal and the input signal,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
331,Write down and explain the Widrow-Hoff learning rule!,"$\Delta w(n) = \mu * x(n) * e(n)$ $\mu = $ learning rate $x(n) = $ input at timestep n $e(n) = d(n) - y(n)$ $d(n) = $ desired signal at timestep n $y(n) = $ output of the network at timestep n The Widroff-Hoff (or delta rule) changes the weights depending on the input and the error, which is the difference between the output of the network and the desired output. This weight change can be scaled by a learning rate.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
332,Write down and explain the Widrow-Hoff learning rule!,The Widrow-Hoff rule is used in error-correction learning and uses the current error and output of the system to determine the new weights. $w(n+1) = w(n)+\eta \cdot e(n) \cdot y(n) $,1,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
333,Write down and explain the Widrow-Hoff learning rule!,"$\Delta w(n) = learning\rate \cdot x(n) \cdot e(n)$, where x is the input data, $e = d - y$ is the error from the desired output and the actual output, and the learningrate is a parameter chosen as necessery to change the speed of learning. $w{new} = w{old} + learning\rate \cdot x \cdot e$, this is the formula to update the weights and to learn the input data.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
334,Write down and explain the Widrow-Hoff learning rule!,"weights(t) = weights(t-1) * learningrate * (desired(t) - output(t)) The Widrow-Hoff rule, also the delta rule, is used to update the weights of neural networks in a learning algorithm. It uses the previous weights' result and compares it to the desired result. This discrepancy is then applied to update the weights based on a learning rate.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
335,Write down and explain the Widrow-Hoff learning rule!,wnew = wold + learningparameter * error(n) * input(n) while error is: desiredinput - currentoutput the new value for the synaptic weight is computed of the old value plus a learning rate times the current error and the input. The output error is decreased in each step until the change is to small or the generalization is sufficient,2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
336,Write down and explain the Widrow-Hoff learning rule!,"Rule: w+1 = w + n * x * ( y - d) where n is the learning rate, x is the input, y is the ouput of the network d is the desired output The widrow-Hoff rule minimizes the error (y-d). The weight change is proportional the ibnput x and the error. It can be derived from steepest descend.",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
337,Write down and explain the Widrow-Hoff learning rule!,,0,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
338,Write down and explain the Widrow-Hoff learning rule!,This the basicly the calulating mean squared error (MSE) from the expected output and real output. Modifiying the weights for Minimizing MSE it .,0,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
339,Write down and explain the Widrow-Hoff learning rule!,Widrow-Hoff rule states that the weight adjustment is proportional to the product of input and the error in the output. It is also called the delta rule. $$\Delta w{ji} = \eta xie{ji}$$ $\eta$ is the proportional constant also called as learning constant $$W(i)=W(i-1)+\Delta W{ji}$$,2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
340,Write down and explain the Widrow-Hoff learning rule!,$\Delta W{ji}$ = $\eta ejxi$ Adjustment made to the weight of a neuron is proportional to the product of the error in that neuron and input applied to the neuron.,2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
341,Write down and explain the Widrow-Hoff learning rule!,"Widrow-Hoff learning rule is derived from LMS error method, and it is defined as: $W{t+1} = W{t} + \mu \cdot \Delta W$, where $\mu$ represent learning rate, and $\Delta W = -(gradient \ of \ instantaneus \ erorr) = -(d - y)X $, Here $d$ represent desired signal, while $y$ represent output signal of a neuron. $X$ represent input of a neuron",2,The adjustment made to a synaptic weight of a neuron is proportional to the product of the error signal and the input signal of the synapse in question. The rule is derived from the steepest descent method.
342,"Explain back propagation, use the correct technical terms!","In backpropagation, the gradient of the error produced at the output layer (by partially differentiating the cost function with respect to the weights) is propogated backwards one layer at a time back to the input layer. This propagated gradient is used to update the weights in the corresponding layer. Backpropagation is necessary because the desired output at every layer is not known and it is only possible to formulate the cost function at the output layer.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
343,"Explain back propagation, use the correct technical terms!","In back propagation, there are two phases: 1. Forward Phase: First we apply input to the network and compute the current output. 2. Backward Phase: We compute the error between current and desired output. Error is minimized by computing gradient of error with respect to weight. In return, weights are adjust. After adjusting weights in backward phase, we again go to forward phase and compute the current output, check whether error is minimized or not.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
344,"Explain back propagation, use the correct technical terms!",Back propagation wants to minimize the error function E. E is given by: \( $ \frac{1}{2}\sum e(n)^{2}$ \). THe error function can be minimized by calculating the gradient starting from the output. Term for calculating the gradient differs. It depends on whether the neuron for which the gradient to be calculated is an output neuron or a hidden neuron.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
345,"Explain back propagation, use the correct technical terms!","Backpropagation is the general form of the delta rule, formulated for networks with multiple hidden layers. Here, we propagate the error of the network back to the input layer to determine the change of weights, using the error signal in the output layer and subsequently the local gradients in the hidden layers. In the forward pass, we compute the net output forwards. In the backward pass, we propagate the error backwards. The BP rule was derived from the error gradient w.r.t. the weights, and application of the chain rule.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
346,"Explain back propagation, use the correct technical terms!",Back propagation is propagation of error from the output layer to the hidden layer in network with multiple layers. This is done by calculating the local gradient of each node and then using this (along with the weight) to determine how much of the error is to be propagated to the particular node,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
347,"Explain back propagation, use the correct technical terms!","Back propagation is used in multi layer network. It consits of two phases: Forward and backward. In the forward phase we give and input to the network and caculate its outputs. Also memorize the local field of each node. The local gradient (delta) is used to adapt the weights of the layers. It is different for output and the remaining layers. For node i in an output layer: $\deltai(vi) = \varphi^\prime(vi)(di - yi)$ For node i in other layers: $\deltai(vi) = \varphi^\prime(vi)\sum{j\in C} wji \deltaj(vj)$, where $C$ are all the nodes that use node i output as an input repeat this process for all input data until error is small enough",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
348,"Explain back propagation, use the correct technical terms!",The back propagation algorithim is there to train a mulilayer feedforward ANN. We change the weights by computing the local gradiant at each neuron by using the neurons in the layer befor. The local gradient of the output neurons can be computed easaly. The activation function has to be differantable for the backpropagation algorithm. In the forwart pass we compute the output y at the output layer. In the backard pass we use the output y and our desired output d to compute the local gradients at the output layer. Then we go back layer by layer and use the local gradients from before to compute the new local gradients. By that we minimize the average squared error function.,2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
349,"Explain back propagation, use the correct technical terms!","Backpropagation is a learning algorithm for Multilayer FF NN. It is supervised error correction learning. The weights are initialised randomly The algorithm has to steps: In the forward pass the the output is calculated by using the current weights. In the backward pass the weight update for the outputlayer is as like in single layer ff. The error is used to update the weights. BP allows us to also calculate the error of hidden layers. For each hidden layer we use a local gradient as the error. The local gradient is the sum of weighted error of the following layer, which is passed trough the derivate of the activation function. So it is possible to backpropagate the error from the output layer to to first layer. A common Problem in BP is the vainshing gradient problem. Depending on the activation function used the local gradient gets smaller in each layer until it is eventually less than the floating point precision used. This limits the number of layers that can be stacked.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
350,"Explain back propagation, use the correct technical terms!","The back propagation algorithm is a learning algorithm for updating in the weights in a multi-layer neural network. For updating the weights of all the layers, the error of each neuron must be calculated. In the back propagation algorithm, two phases will be defined: - Forward phase: the output of the neural network will be calculated and also the error of the neurons in the output layer. - Backward phase: the gradient of each neuron will be calculated, by using the calculated error on the output layer and the defined connections between the hidden layer and the output layer. If multiple hidden layers are defined, the error will be iteratevely will be given backwards and the weights at each neuron will be updated.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
351,"Explain back propagation, use the correct technical terms!",Back propagation is a steepest decent method that uses the final produced error and the local gradient to define the amount of change needed for each synaptic weight. In this method we have two phase: - forward phase: in this phase we feed the input to the network and the network calculate the output - backward phase: in this phase we first calculate the error and then use the local gradient to propagate the error to the network from the last layer to the first and manipulate the synaptic weights,2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
352,"Explain back propagation, use the correct technical terms!","Back propagation consists of two steps: 1. forward pass - data is passed through the network and weights are atapted 2. backward pass - by using local field of each neuron error signal is propagated backward by using local field of each neuron from end to beginning and stacking them up. Local field is partial derivative of the output signal of a a neuron, for output neuron it is simplest to calculate as it has only desired output and actual output to deal with.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
353,"Explain back propagation, use the correct technical terms!","Backpropagation is a learning algorithm in multi layer networks that consists of two phases, a forward pass and a backward pass. In the forward pass, the output is calculated by passing activations layer through layer starting from the input, then through hidden layer and finally output. Then the error is calculated in the output layer and propagated backward through the network. In the forward pass, the weight do not change. In the backward pass, the weights change in proportion to the local gradient.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
354,"Explain back propagation, use the correct technical terms!","Backpropagation is a neural network based learning algorithm where the network learns by propagating the error through the network. BP consists of two stages: + Forward pass: where the error is computed by feeding the input to the network. + Backward pass: where error is propagated through the network for doing the weight updates locally. Since BP has vanishing gradient problem, it is useful to use activation functions which are infinitely differentiable such as sigmoid function.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
355,"Explain back propagation, use the correct technical terms!",The back propagation algorithm is used to calculate the error contribution of each neuron after a batch of data is processed. Required is a known desired output of each input value. Thus the back propagation algorithm is a supervised method. The algorithm can be subdivided into two phases: 1) Propagation: * Propagation forward through the network to generate the output value(s). * Calculation of the cost error term. * Propagation of the output activation back through the network usin the training pattern target in order to generate the deltas (differences between desired and actual output) of all output and hidden neurons / by recursevliy computing the local gradient of each neuron. 2) Weight update: For each weight the following steps need to be applied: * The weight's output delta and input activation are multiplied to find the gradient of the weight. * A ratio (percentage) of the weight's gradient is substracted from the weight. This ration is also referred to as the learning rate and influences the speed and quality of the learning. Learning is repeated for every new batch until the network performs adequately.,2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
356,"Explain back propagation, use the correct technical terms!","Backpropagation is an algorithm for training a neural network, and it contains of two main stages. The first stage is to compute the actual output given the input; in this stage, the signal flows forward from the input layer to the output layer, and the synaptic weights are fixed. The second stage is to update the synaptic weights by propagating the error signals backward from the output layer in a layer-by-layer manner; for each neuron, the local gradient, the partial derivative of cost function to the local field, is computed.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
357,"Explain back propagation, use the correct technical terms!","Back propogation usually occurs in a multi layer perceptron. It uses a non linear activation function. Basic elements: 1. Functional signals: These are the input signals, which passes through the network from left to right. As the name denotes it performs a usefull function at the output of the neuron and another reason for the name is that the functional signals are calculated based on the parameters and the activation function. 2. Error signals: Error signals propogate usually in the reverse direction which contains the error based on the desired output. It consists of 2 phases: 1. Forward phase: In the forward phase the signals propogate from left to right. Weights are fixed and passes through all the layers of the network, that is undergo all the activation. 2. Reverse phase: In the reverse phase, the local gradients are calcualted and are propogated through in the backward direction. Here weights change.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
358,"Explain back propagation, use the correct technical terms!",Backpropogation is used for training multi layer networks. It constitutes of forward pass and backward pass. In forward pass network computes the output. Based on this the errors are calculated based on difference between network output and desired output. These errors are the backpropogated to network during backward pass and used for adjusting the synaptic weights.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
359,"Explain back propagation, use the correct technical terms!","HERE: Backpropagation is used for Multilayer perceptron network. It consists of two passes. - Forward pass: The outputs are calculated at every computational node and passed till the output node where the error is calculated by difference of desired output and the actual output. In this pass, the weights of the synaptic links are not changed. - Backward pass: The error generated at the output neuron is passed in the backward direction i.e., against the direction of the synapses and the local gradient of the error is calculated at every neuron.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
360,"Explain back propagation, use the correct technical terms!","Back prop is a way of training a neural network by adapting the weights using error produced. It consists of two phases, forward and backward. Forward phase computes the output along the network using the function signal. In the backward phase, the error of thr outpur fromthe derired output is computed and a local gradient of the error is used to update the weights iof the network. The local gradient considers the credit or blame of the corresponding weights of neuron in producing the output.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
361,"Explain back propagation, use the correct technical terms!","The back propagation algorithm is based on the error correction learning rule and consists of two passes: 1. Forward pass : The input signal applied to the source nodes of the network is propagated forwards through the different layers of the network, and the output is computed at the output layer of the network. 2. Backward pass : The error signal computed at the output is propagated backwards, with a local gradient computed at each of the hidden layer neurons, in order to adjust the synaptic weightsof the neuron in the network.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
362,"Explain back propagation, use the correct technical terms!",Back propagation is moving the error backwards recursively through the network by calculating the local field of every neuron to update the weights. It is based on the chaining rule of derivatives.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
363,"Explain back propagation, use the correct technical terms!","Backpropagation is a neural network which has two stages: -Forward pass: In forward pass the error is calculated in the output layer with the help of the desired output and the given output. e = d - y - Backward pass: It begins in the output layer , in this case the error is passed backwards with the calculation of gradients at each layer of the neural network So in back propagation the adjustment to weights is made based on the local gradients which is calculated at each layer.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
364,"Explain back propagation, use the correct technical terms!","It contains forward pass and backward pass. In the forward pass, input is applied to the network and propagate it forward through the network, then compute the output of neurons in output layer and errors for output neurons. In the backward pass, compute local gradients and update the synaptic weights according to error correction rule for each neuron layer by layer in a backward direction.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
365,"Explain back propagation, use the correct technical terms!",Back-propagation algorithm consists of two passes:<br> 1. Forward pass: the input vector is applied to the network layer by layer 2. Backward pass: the weight is adjusted based on error correction learning rule. <br> <br> Back propagation uses error correction learning rule and the objective is to minimize the average of squared error.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
366,"Explain back propagation, use the correct technical terms!","* Backpropagation is a steepest decent method that calcualtes the error at the output neurons and backpropagates those errors backwards to update the weights of each neuron. * The synaptic weight updated is directly proportional to **partial derivatives** * Local gradient is calculated at ouput neurons and hidden neurons. * Local gradient at output neurons are calculated using the observed error. * But the error function is missing in the hidden neurons, so the local gradient of hidden neuron j is calculated recursively from the local gradients of all neurons which are connected directly to the hidden neuron j.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
367,"Explain back propagation, use the correct technical terms!","Backpropogation has 2 steps. Forward pass: In forward pass the data is run through the network and the error is calculated. Backward pass: In Backward pass the weight is adjusted using local gradient of error such that the error is minimized. There are many ways for weight adjustment like, steepest descent, Newtons method, Gauss newton method.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
368,"Explain back propagation, use the correct technical terms!",The back propagation is a learning method in neural networks. Back propagation enables the feed forward netwowrk to represent XOR gate. It has two phases: forward pass: the initial weights are used to calculate the value of the output neuron backward pass: starts from the output layer and travels backward. During this phase the weights are changed based on the local gradients of each neuron|,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
369,"Explain back propagation, use the correct technical terms!","Back propagation is used to learn weights in a multi-layer feed forward network. It is divided into two steps: forward and backward. In the forward step one input is passed through the network to calculate the output of the network. This output is used to calculate the error of each output neuron given the desired output. After this forward step, in the backward step the weights are changed beginning in the end of the network. Each weight is changed by taking the derivative of the activation function of the neuron times either the error, if the following neuron is an output neuron, or all local gradients of connected neurons times the corresponding weights. The weight changes are the local fields.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
370,"Explain back propagation, use the correct technical terms!","Backpropagation is used in Multilayer Perceptrons to give a method of adapting the weights. First the forward phase is run like in a regular feedforward network. Then after the output and thus the error is determined the error is backpropagated from ouput layer through the network. Since we have multiple layers, there is only a desired output of the network for the last layer. To counteract this problem a gradient is calculated for every neuron during the backward pass. The gradient is giving a measure of the contribution of this neuron to the final error. The gradient is then used to update the neurons weights. If the neuron is not part of the output layer, the previous gradients are used to calculate the new gradient instead of using the error.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
371,"Explain back propagation, use the correct technical terms!","Back propagation consists of two steps: 1. step - Forward pass: Here the input data is fed into the network and the output is calculated at the output nodes. The usual calculations of the induced local field are done by using this formula $v = \sum wx + b$. The output is then calculated using this formula $y = f(v)$, where f() is the activation function. 2. step - Backward pass: Here the error is backpropagated through the network from the output layer to the input layer. In the output layer the error is calculated using this formula $\delta = d - y$, using the desired output d and the actual output y. In the layers before the output layer the local gradient is used to calculate the error using the error from the output layer $\delta = w\delta x$. Additionally the weights are updated using $w{new} = w{old} - learning\rate \cdot \delta x$",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
372,"Explain back propagation, use the correct technical terms!","Back propagation is a learning algorithm for multilayer neural networks. At first, the input is propagated through the network until the end is reached. Here the error is calculated with the desired result. Then the error is used to update the weights from the back to the front. For the output layer the weights can be updated directly with the calculated error. The following layers have to use the local gradient of the previous error, which is calculated with the derivative of the activation function and its error. This is then used to update the weights and repeated until the front is reached.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
373,"Explain back propagation, use the correct technical terms!",back propagation is used in multilayer feedforward networks. first the forward pass is computed. The given error at the output nodes is used to compute the weight changes using widrow-hoff learning rule. then the error is given back layer by layer in the backward pass to compute the error and weight changing for each layer recursivly. The learning can be done in sequential (online) or batch mode (offline),1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
374,"Explain back propagation, use the correct technical terms!",In multi layer ff networks the error is only available in the last layer. Therefore the error is propagated back through the network using the backpropagtion algorithm. In order to do so the local gradient has to be calculated. Update of the weight: w+1 = w + n * x * gradient where the iput x is the output of the previous layer. The local gradient is calculated diffrently depending if the neuron is in the output layer or in the hidden layer. Output layer: $ gradient = phi`(x) * (y -d) $ Hidden Layer: $ gradient = phij`(x) * SUM(wi * local gradienti) $,2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
375,"Explain back propagation, use the correct technical terms!",In steepest gradient weights are adjusted in decreasing direction of error function. But for hidden neurons there is no labels available to calculate the error. Hence final ouput error is backpropogated through the layers inside the hidden layers of NN. This is possible with continuous activation function and chain rule on its derivatives. Final error is differentiated with respect to hidden weights. Chain rule is applied to find local error on hidden neurons.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
376,"Explain back propagation, use the correct technical terms!",the back propagation algorithm it consist of forward pass and backward pass computes the output of the neuron then it propagates in backward direction while recursively compute local gradient of the neuron weights are adjusted accordingle.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
377,"Explain back propagation, use the correct technical terms!","Back Propagation is the process of learning in Multi Layer Perceptron in which the error from, the output of the network is fed back into the network to adjust the weights in the hidden layer. That is the error back prpagates into the network to enable the network to learn by adjusting the synaptic weights based on it.",1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
378,"Explain back propagation, use the correct technical terms!",* Back propagation is a process to make adjustment to the weights of a neural network in a way that minimizes the average squared error of the training data. * It uses steepest decent method. In each step it moves towards the direction that gives maximum decrease of the error. * In back propagation the error is propaged backward from the last layer towards the earlier layers. The adjustments made to the weights is proportional to the partial derivative of the error with respect to the weight. * The partial derivative is calculated using repeated application of the chain rule.,1,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
379,"Explain back propagation, use the correct technical terms!","The idea of back propagation method is to propagate error from ouput (final) layer backward to hidden layers, and adjust the weighs of neurond in hidden layer, based of this error. This is required because we do not have error information for hidden layers, only for output neurons. The error from output layer is propagated to hidden layers using idea from steepest descent method. Namely, local gradients are computed for each neuron in backpropagation, and these local gradients define how error changes, in terms of weights. Local gradients are derived from chain rule for each layer. The fact that local gradient for each hidden layer is derived based on local gradient of a previos layer, defines that as we propagate more and more in hidden layers of NN, the gradient of a error function vanishes, which means that as we go deeply back in NN, the change in weights is becominng smaller and smaller. This is a drawback of back propagation method.",2,Backpropagation lowers the error of a MLP level by level recursively backwards. It back propagates an error from the last layer to the first layer by updating the weights. The updates are determined by the local gradient at each level which is computed by partial derivatives of the error and chain rule.
380,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","Learning rate controls the speed of the descent. When learning rate is low, the weight updation is overdamped and convergence is slow. When the learning rate is high, the weight updation is underdamped and a zigzagging behaviour is exhibited in the weight space. When the learning rate is too large, learning becomes unstable.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
381,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","If learning rate is very smaller, then transition are over-damping, trajectory of weight vector follows the smooth path. If learning rate is large, then transition are under-damping, trajectory of weight vector exhibits the zigzagging(or oscillatory) behavior If learning gets higher than some threshold, then learning algorithm gets unstable or diverges",1,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
382,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",Learning rate n determines stride of delta of weight. If learning rate is too large weights starts to ziggerate.,1,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
383,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","When training with SD, the learning rate determines the step size we take towards the negative gradient. When the learning rate is too small, the weights may be overdamped and reach the error function minimum slowly, eventually getting stuck in local minima. When step size is too big, the weights may be underdampened, bouncing between ridges of the error surface and never find the minimum (especially when the minimum is in a steep ravine of the error surface)",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
384,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","- Learning rate is used to control how much the wright update is affected by the error correction or so on. - Learning rate too low: Learning is slow and takes more time - Learning rate too high: Learning is fast, but causes zigzagging behaviour in convergence. - If the learning rate is too high, it may result in situations where the zigzagging behaviour will cause it to overshoot, and may never finally converge.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
385,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",The learning rate defines the speed of the weight change. A learning rate too high can lead to oscillation around the optimal weight such that its never reached. A learning rate to low results in very slow learning and slow convergence.,1,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
386,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",The learning rate is needet to make the algorithm more stable. A high learning rate makes the weightchanges zickzacking and the algorithm might not converge A low learning rate makes the path in the W-plane more smooth. If the learning rate gets to a certan critical value the algorithm might not converge at all,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
387,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning reate is a factor of how much we trust the datapoint. Normally it is in the range of [0,1]. A high learning rate normally results in a faster convergence while a lower rate in a slower conversion. If the rate is choosen to high, it is possible that the cost function diverges. If the rate is to slow it is possible that the rate so conversion is so slow that we never reach a local minimum.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
388,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate is a parameter using on updating the weights in a given iteration. This parameter represents the importance that is given to the adaptation of the weights. So when setting the learning rate small, the learning machine will learn slower but also in a more stable way. On the other hand, when setting the learning rate with a large value, the learning machine will learn faster but in an unstable way. The danger here, is that depending on the learning rate's value, the algorithm may never come into the perfect value. If the learning rate is too small, it may land into a local minimum and never approach the global minimum of the function. If the learning rate is too big, the learning progression will have a zig-zagging behaviour and never approach the ideal value.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
389,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",- The learning rate defines the size of steps that the method moves in the search space. - If the learning rate is too small the method needs to take huge number of steps and maybe it stuck in a local minima - If the learning rate is too big the method will converge very fast toward the global minima but there is a probability that it oscilates around the global minima and never reachs it,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
390,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","If learning rate is to large, then proccess will oscillate a lot and might not converge. If learning rate is to small, then convergance will happen very slowly",1,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
391,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate tells us how confident we are of the error, and it affect the convergence rate. A low learning rate will slow the convergence, making the system overdamped. A high learning rate will speed the convergence but the value oscilates, making the system underdamped. The system can become unstable if the learning rate is above a threshold value.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
392,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","+ If the learning rate is too small, then the system is overdamped and the algorithm takes a long time to converge. + If the learning rate is too large, then the system is underdamped and the algorithm oscillates around and optimal solution or could potentially make the system unstable.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
393,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The steepest descent method is an algorithm for finding the nearest local minimum of a function which presupposes that the gradient of the function can be computed. The method of steepest descent starts at a point P0 and as many times needed moves from Pi to P(i+1) by minimizing along the line extending from Pi in the direction of gradient f(Pi) the local downhill gradient. The danger of the algorithm is, that it can get stuck in a local minima.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
394,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate determines the rate of learning: the smaller the learing rate is, the slower the learning process is, but the path of weight adjustment is smoother. The larger the value is, the faster the learing process is, but it can result in oscillation and instability.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
395,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate is $\eta$ So based on the learning rate, it undergoes various oscillation. We could see zigzagging behaviours. 1. When the learning rate is large, the system is said to be under damped. 2. When the learning rate is small, the system is said to be over damped. Here we can see a zigzagging behaviour towards the convergence phase. 3. After the learning rate crosses a certain value it becomes unstable. It may stuck in a local minima which is considered to be another danger",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
396,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",Learning rate in steepest descent can directly affect the convergence of the algorithm. If the learning rate is very small then algorithm can take long time to converge i.e response is ovderdamped. But if the learning rate is amde very high then we may observe zig-zagging (oscillatory) behaviour and sometimes algorithm may fail to converge (underdamped response).,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
397,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","HERE: - When the learning rate is small, the learning is very slow. - When the learning rate is large, the learning is unstable and can exhibit zigzag behavior. - When the learning rate is too large, the learning never converges.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
398,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate defines the efficiency of learning machine. If it is small, the system response may be overdamped, if large , the response may be underdamped and if it exceeds a critical value, the response may diverge. The danger is the possibility of the system output to not converge. This should be ensured by scaling the learning rate using the largest eigen value of the correation matrix of the input.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
399,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The value of the learning rate parameter $\eta$ controls the speed of descent and convergence towards the optimal weight vector. For small values of $\eta$, the transient response of the algorithm is overdamped and the weight trajectory follows a smooth path. On the other hand if the value of $\eta$ is large, the transient repsonse of the algorithm is underdamped, and the weight trajectory follows an oscillatory path in the W-plane.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
400,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","Learning rate $\eta$ has a profound impact on the learning in steepest descent. 1. If $\eta$ is too small, the system is underdamped and convergence is slow. 2. For larger $\eta$, the system is overdamped and tends to oscillate. 3. If $\eta$ exceeds a certain critical value, steepest descent may even diverge!",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
401,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",Learning rate has huge impact on convergence of the network. If the learning rate is low then the transient response of the algorithm is overdamped and the trajectory of w(n) is smooth. If the learning rate is high then the transient response of the algorithm is underdamped and trajectory of the w(n) is zigzag. If we choose the wrong learning rate then the network might not converge.,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
402,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","learning rate controls the speed and convergence of steepest descent method. 1. if it is small, the trajectory of weight vector follows a smooth path in W plane; 2. if it is large, the trajectory of weight vector follows a zigzaging path; 3. if it exceeds a critical value, then the algorithm is unstable.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
403,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",1. Large learning rate $\eta$ results in a zigzagging behavior but it can converge quickly. 2. Small learning rate $\eta$ results in a smooth behavior but it is slow to converge,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
404,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","* Learning rate tells the network that how much steps it should move towards direction opposite to the gradient vector. * If the learning rate is too large, the weight updation will be high. * So the danger is, learning may oscillate or the network overfit the data.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
405,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",Learning rate is used to regulate the speed of learning. If the learning rate is small then the learning is slow and if the learning rate is high then it oscillates. If it exceeds the critical value then the algorithm is unstable.,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
406,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",Learning rate is used to decide how fast the network should converge during the training phase If the learning rate is too high - the system oscillates and becomes overdamped too low - the system becomes underdamped and learns very slow,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
407,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate tells how long one step in the method of steepest descent is. If the learning rate is too high, the learning will oscillate and may not converge. If the learning rate is too small the convergence will take many iterations.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
408,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","If we use steepest descent we use the learning rate to adjust the speed of the convergence to a minimum error. If the learning rate is too small, the learning is going on rather slow. If the rate is high, the error is zigzagging on the error surface towards the minimum. If the learning rate is to high, it might not converge but diverge.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
409,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate is a value between 0 and 1, which determines how fast the network learns. When using small values for the learning rate, the network converges slowly and needs alot of processing. When choosing big values the learning oscillates and becomes unstable. The goal is to choose the learning rate in a way that it does not learn to slow, which needs more input data for convergence, and that it does not become unstable.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
410,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate defines the speed of the learning convergence. High values lead to faster learning und low values to slower learning. However, high values can lead to oscillations in the learning space and may overshoot the desired result and never reach it.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
411,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate gives the speed of learning. It defines the stepwidth in direction of steepest descent. If the learning rate is small, the learning is more stable but slower. When it is high, the learning is more unstable but faster. The danger is to overcome a minimum and result in oscillating behaviour",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
412,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",A too small learing rate can lead to a very slow convergence or to no convergence at all if the time learn becomes too long. A high learning rate can lead to an oscillating behavior and prevent convergence.,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
413,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",Learning rate is a scalar multiplied with adjustment term to adjsut the weights. It ensures the rate of learning. It is typical greater than 0 and less than equal to 1. It govers the rate of sliding alond the curve towards the minima. 1. Lower learning rate will result in slow learning but chances of finding optimal minima are greater. 2. Higher learning will result in hopping on either side of minima hence zigzag behaviour. 3. Very high learning may not converge.,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
414,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",if the learning rate is large then the it follows the zizag motion. if the learning rate is too low then it takes time for converging . if the learning rate is very large or critcal then it becomes unstable. while processing there is possiblity that it will get stuck in local minima.,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
415,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","When using steepest descent the learning rate($\eta$) determines the speed at which the weihts are adjusted in the NN. There can be two possible danger related to leraning rate depending on its magnitude: 1. Low learning rate(eg, $\eta = 0.01$) results in smooth variation of the weights but makes the process becomes slow. 2. Hight learning rate (eg, $\eta = 0.01$) results in faster weight adjustment but it leads to an oscillatory nature in the learning which is unwanted.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
416,"When learning using steepest descent, explain the role of the learning rate? What is a danger?",* With a small learning rate the network will converge very slowly towards the optimal weight of the network but it will give better perfomance in generalization. * With a high learning rate there can be zigzag effect. because of the large rate the network may miss a local minima and jump to a higher point. * With a very high learning rate the network may become unstable.,2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
417,"When learning using steepest descent, explain the role of the learning rate? What is a danger?","The learning rate defines the speed of steepest descent search for min of a error funtion. In other words, it defines how strong the change in weights will be, throughout optimization procedure. Higher learning rate, faster learning, but then learning is characterized by oscilations in searhc for min. This is dangerous because if learning rate, becomes bigger that a certain value, it can make search with steepest descent unstable. IN this case steepest descent will start to diverge, istead of converging to min. In other case, when learing rate is small that lerning is slower but safer, and the learining path is not oscilatory.",2,"Learning rate controls the speed of the convergence. When the learning rate is low, the convergence is overdamped and slow. When the learning rate is high, the convergence is underdamped and follows a zigzagging path. When the learning rate exceeds a critical value learning becomes unstable. "
418,How does a Reduced Boltzman Machine work (main idea)?,The reduced blotzman machine works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.,2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
419,How does a Reduced Boltzman Machine work (main idea)?,It is a recurrent network. It opreates by flipping. It has two groups of neurons: Visible neurons and hidden neurons. Visible neurons provides interaction between environment and network. Hidden neurons are running freely. It has two modes of operation: . Clamped State: states of the neurons are clamped. . Free running state: Neurons are running in free condition,2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
420,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
421,How does a Reduced Boltzman Machine work (main idea)?,"RBM implement a combination of graphical and probabalistic ideas, using probabilites of activations inspired from energy based networks. We present a training input to the RBM, and determine the hidden activations based on a probability of net input and edge weights. Then, when unclamping the training data from the network, sample from the distribution of the hidden layer, where the RBM tries to rebuild the distribution of the input data. RBM may be used for data completion or denoising, where e.g. incomplete images are complted based on the learned probability distribution.",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
422,How does a Reduced Boltzman Machine work (main idea)?,"RBM has two layers and are interconnected (recurrent) operates by flipping the internal states (+/- 1)> Unlike the boltzmann machine, reduced boltzmann machine does not contain interconnections among the same layer. The weight update is done by the differnce in correleation in clamped and free running mode.",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
423,How does a Reduced Boltzman Machine work (main idea)?,It consists of only two layers: input and hidden layer. During training data is presented to the input. The hidden layer starts oscillating.,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
424,How does a Reduced Boltzman Machine work (main idea)?,"The Reduced Boltzman Machine is an stochastical recurrent ANN, that operates with two classes of neurons : hidden and visible. It operates by neuron-flipping with a probability impacted by the neurons arount. So it uses the hebbian rule. An Reduced Boltzman Machine can learn the classify data and can repoduce the learned patterns.",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
425,How does a Reduced Boltzman Machine work (main idea)?,"The structed of RBM is a bitpartied graph. It uses hebbian learning for training and the neurons used are binary stoachastic neurons, which have a binary state, which fire based on a probability. The training is achived by passing the information a many times between the hidden layer and the input layer. There weightsare updated on the pass into the hidden layer. Weigths between input and activations in the hidden layer are increased, weights between gernerated inputs of the rbm and the hidden layer are decreased.",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
426,How does a Reduced Boltzman Machine work (main idea)?,"The main idea of an RBM can be defined as follows: - Two layers will be defined, where each neuron will be connected to every neuron of the other layer. - The input will be passed from the first layer to the second one, and the state of each neuron of the second layer will be calculated. - The neurons with active states will pass again its values to the input layer. - The values given from the second layer will be compared with the input values, and with the two states, the weights will be adjusted.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
427,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
428,How does a Reduced Boltzman Machine work (main idea)?,"They are neural network with only one hidden layer, neurons from input to hidden layer are fully connected, neurons from hidden layer to output layer are fully connected as well.",0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
429,How does a Reduced Boltzman Machine work (main idea)?,"The Reduced boltzman machine works by flipping neurons. It can operate in clamped or free running state. - If two connected neurons are activated at the same time, the weight is increased. - If any of the two neurons are fired asynchronously, then the weight is reduced or removed.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
430,How does a Reduced Boltzman Machine work (main idea)?,+ Reduced Boltzman Machines (reduced because inputs do not share information via synapses) are one of the initial NNs which consists of input layer and hidden layer. The system adapts its internal weights and tries to reproduce the inputs.,1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
431,How does a Reduced Boltzman Machine work (main idea)?,"A RBM is a shallow two layer network containing a visible and a hidden layer. Each noden in the visible layer is connected to each node of the hidden layer. It is considered as restricted, because no two nodes of one layer share a connection. A RBM is the mathematical equivalent of a two way translator. In the forward pass a RBM takes the inputs and translates them to a set of numbers that encode the inputs. In the backward pass it takes the set of numbers and translates them back to form the reconstructed inputs. A well trained RBM will be able to perform the backward translation with a higher degree of accuracy. Three steps are repeated over and over through the training process: 1) Forward pass. 2) Backward pass. 3) Evaluate quality of reconstruction as visible layer (often solved with KL divergece)",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
432,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
433,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
434,How does a Reduced Boltzman Machine work (main idea)?,RBM is an unsupervised learning technique. It has visible neurons and hidden neurons. Neurons are in either +1 or -1 states. It uses the idea of simuilated annealing to flip the neuron states based on energy function and pseudo temperature. It operates in 2 states - clamped state and free flowing state. In clamped state only hidden neurons are flipped and in free flowing state both visible and hidden neurons are flipped. Weights are adjusted based on avergage correlation difference between all the neurons in clamped and free flowing state.,2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
435,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
436,How does a Reduced Boltzman Machine work (main idea)?,"RBMs work on the principle of binary states, free-running or clamped. The weight update is done based on the Botlzmann's formula using the pseudotemperature, which gives the proobability of error.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
437,How does a Reduced Boltzman Machine work (main idea)?,"The Reduced Boltzman Machines function by using two types of neurons : visible neurons that provide an interface between the environment and he network, an hidden neurons that operate freely. The learning can proceed under two conditions, namely: 1. Clamped state : where the visible neurons are clamped to a particular state of the environment 2. Free running state : where both visible and hidden neurons operate freely. If $\rho^+{ij}$ indicates the probability of correlation between the states of neurons i and j in clamped state, $\rho^{-}{ij}$ indicates the probability of correlation between the states of neurons i and j in free running state, then the weight adjustment $\Delta w{ij} = \eta (\rho^+{ij} - \rho^{-}{ij})$",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
438,How does a Reduced Boltzman Machine work (main idea)?,"A Reduced Boltzmann machine (RBM) consists of two layers of neurons: visible and hidden. The neurons may only have two states i.e. activated or not and they flip according to a certain probability based on the weights and states of other neurons. The RBM has two modes: 1. Clamped: The visible layer is clamped to a certain input while the hidden neurons are allowed to change state until the network settles. The correlation in this state is given by $\rho{ij}^+$ 2. Free-running: In this state, the network is allowed to flip all neurons until it settles. The correlation is $\rho{ij}^-$ The weight update rule is given by $$\Delta w{ij} = \eta (\rho{ij}^+ - \rho{ij}^-)$$",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
439,How does a Reduced Boltzman Machine work (main idea)?,Boltzmann machines is a neural network having recurrent structure.It is in two states either on which is +1 or off which is -1.The energy function is given by $E = 1/(1+exp(-delta E/Temperature))$ The state of the input x is turned from +1 to -1 based on the change of the energy deltaE and the pseudo temeperature T.,1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
440,How does a Reduced Boltzman Machine work (main idea)?,"The neurons operate in a binary states, ""on"" or ""off"". In clamped condition, all visible neurons are clamped into specific states by the environment; in free running condition, all neurons including visible and hidden neurons operate freely.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
441,How does a Reduced Boltzman Machine work (main idea)?,It uses an energy function to oversee the learning process,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
442,How does a Reduced Boltzman Machine work (main idea)?,Reduced boltzman machine work based on **flipping operation** and calculating the probability invariances of clamped state and freely running state.,1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
443,How does a Reduced Boltzman Machine work (main idea)?,RBMs run on boltzmann learning rule. The neurons have 2 modes of operation clipped and free running. All the neurons are binary units. Their status can be changed by flipping. All the neurons that are in on position are clipped together.,1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
444,How does a Reduced Boltzman Machine work (main idea)?,It has the structure of recurrent neural network. It has two layers of neuron visible and hidden. the neuron can store only binary values they work based on flipping theere are modes free running and clamped the weights are changes based on the correlation of the neurons in the free running mode and clamped mode,1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
445,How does a Reduced Boltzman Machine work (main idea)?,"In a Reduced Boltzman Machine there are one visible and at least one hidden layer. The visible layer is the input and acts as output at the same time. For each input the neurons of the visible layer will be assigned with a value. With their weights, hidden neurons may either be activated or not. Once the input has been passed through the hidden layers, the values are passed all the way back to the visible layer. For this, different weights are used since the values move in the opposite direction.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
446,How does a Reduced Boltzman Machine work (main idea)?,"In RBMs there are two states, the free running and the clamped state. During the clamped state, the input neurons are clamped to the output neurons. While the network is clamped the probabilities of the Hidden states to be in a certain state are calculated to determine a probability of the output to be correct.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
447,How does a Reduced Boltzman Machine work (main idea)?,The Reduced Boltzman Machine hast an input layer and a hidden layer. Each neuron has a state and a probability to turn on. If the neuron turns on the data passes trough it and the weights are updated. The probability of turning on is calculated by the network.,1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
448,How does a Reduced Boltzman Machine work (main idea)?,"Two fully connected layers, one input and one hidden layer are used. The input layer is the only connection to the environment. The RBM has a specified energy level which can not be changed. However the distribution of this energy to the nodes can be changed. Based on the data input every node has a chance to flip based on its input connections.",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
449,How does a Reduced Boltzman Machine work (main idea)?,the binary state of each neuron is flipped by a given probability. Stochastical learning,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
450,How does a Reduced Boltzman Machine work (main idea)?,Neurons have to states e.g. on or off. Each neuron has a probability to flip from one state to another.,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
451,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
452,How does a Reduced Boltzman Machine work (main idea)?,the main idea of the RBM is compute the Least mean square error of the difference between expected output and real output.,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
453,How does a Reduced Boltzman Machine work (main idea)?,,0,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
454,How does a Reduced Boltzman Machine work (main idea)?,"* It is a Recurrent neural netwokr * It uses two groups of neurons, hidden and visible * It process the training data by flipping the neurons",1,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
455,How does a Reduced Boltzman Machine work (main idea)?,"Reduced Boltzman Machine is a biparted (two parts) Reccurent NN, that has two layers visible and hidden layers. In Reduced Boltzman Machine neurons can have two states, namely, + or - 1, depening on current time step. At each time step, the states of neurons are flipped. Here the visible layer represent interface for connection between evironment and hidden layer, and it operates in clamped mode (limited values by environment). WHile hidden layer, operates in free mode.",2,The reduced Boltzmann machine is a bi-parted graph which works by flipping the states of binary neurons based on a probability determined by the activation produced at the neuron. Neurons are arranged in a visible and a hidden layer in a recurrent fashion. There are two states involved called the clamped state in which the visible neuron is connected to the input and a free running state in which both layers run free.
456,"Define: Echo State Network (ESN), how are they different to FF NNs?",Echo State Network is a type of Recurrent Neural Network and has atleat one cyclic (feedback) connection. ESN consists of a dynamic reservoir and a output layer with neurons. The dynamic reservoir consists of randomly initialized neurons with random sturcture and connections (with atleast one feedback connection). The output layer combines the dynamic behaviours of the reservoir in a required fashion. Only the weights of the output neurons are updated while learning. An ESN consists of feedback connections while a FF NN does not. An ESN could have persisting activations even when there is no input which is not the case in FF NN. An ESN can approximate dynamic systems while a FF NN cannot.,2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
457,"Define: Echo State Network (ESN), how are they different to FF NNs?","Echo State network are recurrent neural network, which means these networks have feedback. While, in feedforward neural networks, there is no feedback. In feedfoward, training data or inputs are not dependent on each other. They do not have any system memory. In ESNs, training inputs are dependent on each other and they have system memory In Echo state network, there are fixed, random generate reservoir weights. These weights are not trained. While, only output weights are trained",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
458,"Define: Echo State Network (ESN), how are they different to FF NNs?",An ESN is a recurrent neural network with many layers and fixed weights. There are several differences. An ESN has a cycle that means witin the network there are backwarded connections. Withn a FF NN there are only feedforwad connections. Within a FF NN all weights are trained. Within an ESN only output weights are trained. An ESN can produce an output without any input. A FF NN needs an input to produce an output.,2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
459,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESN are different to FFNN in so far that they consist of a reservoir of hidden neurons, which may be connected recurrent, as opposed to having a feed forward architecture. Here, the inputs are connected to the recurrent dynamic reservoir, whereas the DR is connected to the linear output layer. The Output layer may be again connected to the DR, whereas during training only weights of the last layer are learned. Weights of the DR of the ESN are thus initialized and never learning, although since have been extended to minimal complexity architectures.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
460,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESN are recurrent neural networls with a large reservoir (or echo chamber) with many nodes (recurrent). The weights are learnt only for the connection between this reservoir and the output layer. The weights are not learnt for the nodes inside the reservoir. The main idea is that during training, the input layer cuases the states inside the reservoir to behave in caertain way, and the weights in the output layer is adjusted to match this and the labelled output. FFNN are feed forward networks, i.e., they do not have any recurrent connections, which is the main difference with respect to ESN",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
461,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESNs are a special class of recurrent neural networks. In contrast to ff they also allow backward node connections and thus are able to memorize data. They are defined by: $xi$ input i, $yi$ output i, a dynamic resaviour, and weights connecting all the components. The dynamic resaviour is generated randomly and fixed. Its topology including weights is never changed. Only the weights between output layer and dynamic resaviour are changed during training. Because the dynamic resaviour allows all kinds of connections between its nodes it can contain memory that is able to remember data. It also has a spectral radius.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
462,"Define: Echo State Network (ESN), how are they different to FF NNs?","An ESN is a recurrent ANN with randome, sparse and fixed interneuron connections in the hidden layers. Just the output layer weights get trained, because the network itself is so complex, it can model very much. If the training was not successful we can just create a new randome ESN. Training an complete ESN would by very complex and would take very very very long. A FF NN is not recurrent (no feedback) and all its weights get trained and most of the time the interneuron connections are not sparsly.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
463,"Define: Echo State Network (ESN), how are they different to FF NNs?","A Echo State Network is a RNN, is has a dynamical reservoir of neurons which are connected with each other and itself. the DR typically consists of more that 100 neurons. The outputlayer consists of linear readouts of the DR. So a neuron in the output layer sums up the weighted behaviours of the DR neurons. The DR is randomly initialised and only the output layer is trained by supevised learning. The main Diffrence is that ESN is a RNN. In contrast to FFNN it can resemble any dynamical system. Usually it is used for time series prediction.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
464,"Define: Echo State Network (ESN), how are they different to FF NNs?","Echo State Networks are a type of recurrent neural networks, where the input layer is interconnected to a reservoir (a random initialized group of neurons with also random interconnections), and this reservoir is connected to the output. The reservoir will not be adjusted, but the output weights. The output weights can also have recurrent connections with the reservoir. The states on the reservoir neurons will be calculated, and with these states and the output weights, the output will be extracted. The main difference with the Feed Forward Neural Networks (FF NN) is that in the FF-NNs there's no recurrency, so the input values will be passed to the next layer.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
465,"Define: Echo State Network (ESN), how are they different to FF NNs?","- In the ESN we have a huge recurrent network which is called ""Dynamic Reservoir(DR)"" and we have an output layer connected to this DR and we will train the network by adapting and manipulating the connection weights just to the output layer - Unlike a feedforward network in a ESN because of the DR we have at least one loops that returns the output of a neuron with some time delay therefore we have memory in our network but in FF NNs we don't have any memory",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
466,"Define: Echo State Network (ESN), how are they different to FF NNs?","Echo State Networks are recurrent neural network type, meaning there are feedbacks in its structure. It is usually only 1% connected. Main difference is that it has a reservoir as a hidden layer where neurons are very randomly connected, with random weight etc. During learning phase only weight outputing neurons are changed. It is required more that 100 neurons to be in a reservoir.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
467,"Define: Echo State Network (ESN), how are they different to FF NNs?","The ESN is a type of neural network model that uses a recurrent neural network as a large, random, fixed dynamic reservoir that remains unchanged during training and only changes the weight of the reservoir to output layer.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
468,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESNs are a form of recurrent neural networks with a least one recurrent input. The ESNs are reservior computers which have memory and can be activated without the inputs. In ESNs, instead of training we evolve the network state by feeding it input sequence. ESNs are different from FF NNs because ESNs contains at least one recurrent connection (feedback).",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
469,"Define: Echo State Network (ESN), how are they different to FF NNs?","The basic idea of ESNs is to use a large, random, fixed recurrent NN (referred to as dynamical reservoir) and to train only connections from the reservoir to the output. The main difference to FF NN lies in the recurrent part of the network, where back passes are built in, giving feedback previous layers. It is not possible to maintain the reservoir beforhand so it suits the given problem. There is a lack of investigation of reservoir construction.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
470,"Define: Echo State Network (ESN), how are they different to FF NNs?","An Echo State Network (ESN) is a modified version of a recurrent network. It has a reservoir, which is a large number of hidden neurons with sparsely-connected random and fixed weights. To train an ESN, only the weights connecting the reservoire and the output layer are adjusted; therefore, the efficiency is better than a normal recurrent network.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
471,"Define: Echo State Network (ESN), how are they different to FF NNs?","Echo state network provides structure and supervised learning for recurrent neural networks. It mainly 1) Directs the fixed, large reccurent neural netorks by providing an input stimuli and also fix a response signals to the neurons which are present inside the reservoir(Pool of neurons) 2) It can be directed to get the desired response by the trainable linear combiner of the response signals. Unlike FF NNs, ESN's have memory. They can be also activated without an input stimuli, whereas in case of FF NN, they require a external stimuli so that they are activated. Also the neurons needs to connected in one full cycle.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
472,"Define: Echo State Network (ESN), how are they different to FF NNs?",Echo State Newtors are type of RNN. It has dynamic reseivoir units which exhibits different dynamics. Weights of these reseivoir units are fixed and are not changed during the training phase. Only the reseivoir to output weights are changed to learn the inputs. These networks converge only if reseivoir units exhibitg echo state property i.e its ouput depends only on the previous inputs. this property is satisfied if spectral norm reseivoir weights is less then 1.,2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
473,"Define: Echo State Network (ESN), how are they different to FF NNs?","HERE: - Echo state networks are recurrent neural networks that have a large resorvoir of oscillator functions that are connected to the input layer. - In FF NNs, consideredthe outputs at the hidden layers are also considered but in ESNs, the ouputs from the reservoir to the final output layer are only considered.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
474,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESN are another implementation of RNNs where training method is completely different. They comprise of a dynamic reservoir with fixed hidden to hidden connections which makes up an RNN with sparse connetivity. Only the output weights which connect the dynamic units and the output of the reservoir are trained using error, unlike RNNs, where the hidden weights are also trained. ESNs are less compuationaly expensive since they can be easiliy trained with experimentation .However, RNNs use much less hidden units compared to ESN for a similar task.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
475,"Define: Echo State Network (ESN), how are they different to FF NNs?","An Echo State Network (ESN) is a neural network that uses a reccurent neural network (RNN) as dynamic reservoir which is not changed during training, and trains only the connection from the dynamic reservoir to the output layer. An echo state network is different from FF NNs due to the presence of feedback connection with the dynamic reservoirs which enables it to maintain activation even without inputs. Each unit within the dynamic reservoir in ESNs are excited differently to different inputs.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
476,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESNs are recurrent neural networks with at least one cyclic connection and are based on the concept of reservoirs. In contrast FF NNs do not have any cyclic connections. Additionally, in ESN the output weights are trained but the reservoir weights are not whereas in FF NNs all weights are trained. The ESN has memory while FF NNs do not have memory.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
477,"Define: Echo State Network (ESN), how are they different to FF NNs?",ESN refers to echo state networks. Echo state networks are the recurrent neural networks where the hidden to hidden layer weights are selected randomly and are fixed and hidden to output layer weights are changed by the learning process.Since ESN is recurrent neural network hence the output echoes throgh the network even when there is no input where as in ff nets there is no feedback so there is no output if there is no input.,1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
478,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESN is a kind of Recurrent NN, which has a large, random , fixed RNN called dynamic reservior and only the weights connecting the reservior and output layer are trained. So ESN combine the desired system function and input/output history echo function.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
479,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESN provides an architetcure of supervised learning principle for RNNs. It is different from FF NNs, because it has a reservoir (based on RNNs) to find a non linear signal response and combine the desired output by a trainable linear combination of these response.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
480,"Define: Echo State Network (ESN), how are they different to FF NNs?","* Echo state networks are recurrent neural netwoks with **Dynamic Reservoirs.** * Weights initialized in the dynamic reservoris will not be updated during training. * Only the weights in output layer (readout states) is updated after each iteration. * In FF NN, all neurons are connected with other neurons in next layer and all the weights are updated in each iteration. * But in ESN, the **neurons are connected randomly** with other neurons and it is **recursive** and the **weights are not updated** in the dynamic reservoir.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
481,"Define: Echo State Network (ESN), how are they different to FF NNs?",ESN is a type of RNN. It has a dynamic reservoir. All the neuron are connected to each other.,1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
482,"Define: Echo State Network (ESN), how are they different to FF NNs?",The Echo state netwrork has a large number of recurrent neural network in them. this set of RNN is called the dynamic reservoir. They can approximate any dynamic model they train the model by changing only the weights of the connection of output of the dynamic reservoir and output of the network FF: they can approximate any continuous function They train by adapting all the weights in the network,2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
483,"Define: Echo State Network (ESN), how are they different to FF NNs?","An echo state network contains of an input layer which is connected to a reservoir, which is a big recurrent network. The output layer is connected to the neurons of the reservoir. While learning in an ESN, only the weights between the reservoir and the output layer are changed, no changes within the reservoir. Differences to feed forwared networks are, that the reservoir is recurrent and that during the training not all weights are changed, but only the ones between ouput layer and reservoir.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
484,"Define: Echo State Network (ESN), how are they different to FF NNs?","In contrast to regular feedforward networks, ESN belongs to the group of Recurrent Neural Networks. It has a regular input layer like the FF, then comes a dynamic reservoir, which is a layer of neurons, where at least one full cycle of connections between the neurons is given. The connections inside this reservoir are not constrained and can thus be any possible connection. This reservoir is randomly initilaized and kept that way. Only the respective connections to the output layer are trained during the learning process.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
485,"Define: Echo State Network (ESN), how are they different to FF NNs?","ESN have an input layer connected to a reservoir, which is a recurrent neural network. The reservoir is connected to the output layer. On the connections to the output layer are weights, which are updated by the network. The weights of the reservoir are chosen randomly and not updated at all.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
486,"Define: Echo State Network (ESN), how are they different to FF NNs?","An ESN is a recurrent neural network, that consists of an input layer, a dynamical reservoir and an output layer. In the dynamical reservoir feedback loops are possible in contrast to a feedforward network. However, this dynamical reservoir is only randomily initialzed and not learned. Only the connections to the output from the reservoir are learned. Normally, in FF NNs all connections are trained.",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
487,"Define: Echo State Network (ESN), how are they different to FF NNs?","echo state networks have dynamical reservoir as hidden layer. The dynamical reservoir consists of recurrent non-linear neurons. Only the linear connections from dynamical reservoir to the output layer are trained. The difference to FF NN is, that the ESN is a recurrent network",1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
488,"Define: Echo State Network (ESN), how are they different to FF NNs?",The core of an ESN is an arbitrary network with recurrence.,0,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
489,"Define: Echo State Network (ESN), how are they different to FF NNs?",Echo state networks have dynamic reservoir with echo state property which is a randomely initialized RNN. Hence it can maintain its own internal state. Which is not possible in FF NN. RNN have feedback connections which ecoes back the state of reservoir as well as previoulsly applied inputs. Hence it can model dynamic systems which not possible with FFNN.,1,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
490,"Define: Echo State Network (ESN), how are they different to FF NNs?",ESN are the RNN recurrent neural network which has at least one feedback cyle. FF NN are normally forward moving networks where the input from one layer is fed into next layer and generated the output . but IN ESN the out put is again fed back as input . ESN is tend to have Resvoir where its randomly connected.,0,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
491,"Define: Echo State Network (ESN), how are they different to FF NNs?","Echo State Network is a type of neural network which has a recurrent network of 100 to 1000 neurons called dynamic reservior, as the hidden layer. The weights are choosen randomly. The synaptic weights from the resorvoir to the output layers are only adjusted during the learning process. They are different from the FF NNs in the following regards: 1. ESN have atleat one loop wheras the FF NNs dont. 2. Only the output weights are adjusted in ESN , in FF NNs both the input and output weights are adjusted. 3. ESN s have a memory, FF NNs dont.",2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
492,"Define: Echo State Network (ESN), how are they different to FF NNs?",* ESN uses a large set of recurrent neurons called reservior. * The weight of reservior neurons does not change after initialization. * The network only lears the weight of reservior to output. * It works very well for one dimentional time series data The Feed forward networks works differently. The input is feed through the network layer by layer and error is propaged backward to make the adjustments till the first layer. In case of ESN the adjustment is made to the reservior to output weight only.,2,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
493,"Define: Echo State Network (ESN), how are they different to FF NNs?",,0,Echo State Network is a type of Recurrent Neural Network and has at least one cyclic (feedback) connection. Only the weights of the output layers are updated while learning. ESN consists of feedback connections while a FF NN does not. ESN can approximate dynamic systems while FF NN cannot.  
494,Describe: the structure on an CNN.,"In a Convolutional Neural Network, the layer order is: 1. Convolutional layer (has kernels which convolve over the input image incase of first layer or feature maps otherwise). 2. Activation layer (ReLU activation). 3. Pooling layer (max or average pooling). These 3 layers can be repeated any number of times. 4. Finally one or more fully connected layers followed by softmax layer.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
495,Describe: the structure on an CNN.,"In CNN, there are mainly three layers: i. Convolution Layer: It is used to capture the low-level and high level features using kernal over the image. ii. Pooling Layer: It is used for dimensionality reduction, and for translation invariance iii. Fully Connected Layer: This layer is same as regular NNs, where all the nodes are fully connected with each other. There is mostly sigmoid activation function is used to compute the probabilities of each output/class. Furthermore, In CNNs, we use Rectified linear unit(ReLU) activation function",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
496,Describe: the structure on an CNN.,"A Convolutional Neural Network has a kernel which is much smaller than the input. This is why it can operate much more efficient than a normal neural network. Normal Neural network O(n \times m), convolutional neural network O ( n $ \times $ k), k is much smaller than m. A convolutional Network operates no large images. The input is preproessed in many layers before it is given to a normal neural network. Preprocessing transforms input into a linear separable problem.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
497,Describe: the structure on an CNN.,"CNN learn on grid data (images, 3d volumes) using filters instead of matrix multiplication. Here, the filters are convoluted with the input in the Convolution layer per neuron, where we slide the filter (defined by fiter size $S\times S$) over the input with a stride (step size), and optional zero padding. Strictly speaking, since for RGB images we are working have three color channels, we work with volumes of filters (For example for RGB images of size $32\times 32\times 3$, a filter of window size $S=5$ has the dimensions $5\times5\times3$). Instead of learning a volume of weights for each convolution step, we share weights, considering that one feature detected in one part of the image may be of interest in another part. Then, we apply a nonlinearity, commonly the ReLu activation, as to introduce nonlinearity into our model. To reduce spatial size of our input, we can either use higher strided convolutional layers or pooling layers, for example the popular max pooling layer, where the maximum value over a subvolume is picked. These layers are then stacked, while in the last layers fully connected neurons are typically used to reduce data to for example a classification vector.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
498,Describe: the structure on an CNN.,A CNN uses convolution instead of matrix multiplication. After this there is a non linearity which may be a function like ReLU. There is also a pooling stage which is used to pool the important features. CNNs are translation invartiant.,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
499,Describe: the structure on an CNN.,"A convolutional neural network consits of convolutional layers. A convolutional layer applies one or multiple kernels (matrix) to an input vector/matrix (typically image) instead of connecting all single inputs of the input vector to the next layer with seperate weights. Instead in training only the kernel is updated. After a convolutional layer there is typicall a pooling layer. Given a window size it reduce the dimensional size of the output of the convolutional layer by using e.g. max or avg pooling. Afterwards the activation layer applies an activation function to the output of the pooling layer. In the end of a cnn there are typically some fully connected regular layers resulting in a softmax activation function, which assigns the probabilities to the classes output.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
500,Describe: the structure on an CNN.,"An Convalutional neuron network assumes the input is an image. Because of that it has a achitecture, so that there are (abwechselnt) covalution and subsampling layers. After the last subsampling layer there is a normal FF NN which classifys the input.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
501,Describe: the structure on an CNN.,"A CNN typically consists of multiple CNN layers and a few fully connected FF Network Layers. I'll assume the fully connected part is not so relevant to this questions. A CNN layer is typically a convolution layer and a pooling layer In the convolution layer a kernel is convolved onto the input. If zero padding is used the result is in the same dimensionality. Depeding on the Kernel the convolution can be 1, 2 or 3D. In the Pooling layer the result of the convolution is reduced to focus ont the importan features. It also helps on translational invariance.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
502,Describe: the structure on an CNN.,"A Convolutional Neural Networks has the following structure: - The input is defined in a grid, so any image or video sequence will be used. - A several number of convolutional layers, where also subsampling (pooling) can be used. - In the convolutional steps a filter will be used for each layer. - After applying multiple convolutional layers, a normal feed-forward networks can be applied, where for example a back propagation algorithm can be used for updating the weights in the numerous iterations.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
503,Describe: the structure on an CNN.,A CNN network consists of: - Input layer - conolution layer - Detection layer - Pooling layer - Next layer(because CNN consists of many layers this will be another block of layers similar to what described),1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
504,Describe: the structure on an CNN.,"Convolutional neural networks are so that first layer is not fully connected but in a way that neuron connections overlap, leading to a grid type structure with overlapping circles. Another layer is connected only with nodes that are responsible for a particular feature (convolutions), then next layer is choosing wich of those convolutions from each ensemble is the most apropriate, after that next layer is fully connected to output neurons.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
505,Describe: the structure on an CNN.,- The CNN has an input layer - The input layer is connected to a convolution layer consisting of three phases: - convolution stage - Detector stage - pooling stage - The next layer (can be a traditional FFNN),1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
506,Describe: the structure on an CNN.,CNNs are feed forward neural networks which replaces matrix multiplication task with convolution operation which is much sparse. CNN contain followng stages: + Convolution (learns local features) + max pooling (coarse-graining to learn better abstraction of input image),1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
507,Describe: the structure on an CNN.,"In comparison to other NN, in CNN matrix multiplication is replaced with convolution. Everything else remains the same.",0,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
508,Describe: the structure on an CNN.,"An CNN (covolutional neural network) contains a set of hidden layers for feature extration (convolutional layers, pooling layers), and fully-connected layers that classifies the features. The covolutional layers are used to carry out the covolution between the incoming signals with a set of filters, resulting in a set of feature maps. The pooling layers are used to reduce the dimensionality of the feature maps, and make the features invariant of rotation or displacement.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
509,Describe: the structure on an CNN.,"A CNN 1. starts with a input, where we perform the convolution, which provides a piece of activation. 2. Next it is being sent through the activation layer otherwise known as the detection layer. 3. Then the final stage is the pooling.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
510,Describe: the structure on an CNN.,In CNN we have different Kernels which are used for extracting certain properties of the inputs. These are called feature maps. After this there is a detection phase which introduces non-linearity. Further there is pooling which introduces translational invariance. There can be many such layers of feature maps and pooling. Finally its reduced to single row input and trained using traditional methods like Back Propogation algorithms.,2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
511,Describe: the structure on an CNN.,"HERE: Convolutional neural networks have 4 main layers where input layer is connected to convolutional and subsampling layers followed by another set of convolutional and subsampling layers connected to the output layer. They are designed to specifically recognize 2-d shapes are invariant to skewing, rotation and the actual location of the object.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
512,Describe: the structure on an CNN.,"CNN comprises of multile layers of neurons which perform specific tasks. The initia layer is the convolution layer which performs convolution of the input with the elements of a given kernel. Simpler tasks such as edge detectoíon are performed. Detector layer forms a seconf layer here the output of convolution layer if fed through an activation function such as ReLU. Further, the data is pooled in the pooling layers where downsamping is done to reduce dimensionaity. These layers are repeated to perform more complex feature extraction operations.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
513,Describe: the structure on an CNN.,A CNN is a neural network that replaces matrix multiplication with a mathematical operation called convolution in one or more layers. The main idea behind the structure of a CNN is to replace the activation of neuron with a flipped filter (Convolution layer) and then apply another function called pooling to adust the output further.,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
514,Describe: the structure on an CNN.,"A CNN consists of several stacked Convolutional layers which can be separated by other layers such as Pooling, Activation, Zero-padding and Dropout which is a form of Regularization. The output layer is generally dependent on the task but could be a Softmax Activation from a Fully connected (also called Densely connected) layer. The number of outputs is usually the number of classes.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
515,Describe: the structure on an CNN.,"The structure is as follows: -Convolution: In this layer convolution takes place instead of matrix multiplication -Deconvolution: In this layer deconvolution takes place , by matrix multiplication -Average weight layer: This is a max pooling layer",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
516,Describe: the structure on an CNN.,"1. first stage, the layer performs several convolution parallel to produce a set of linear activation 2. detector stage, each linear activation is run through a non-linear activation 3. third stage, use a pooling function to modify the output of layer.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
517,Describe: the structure on an CNN.,1. Convolution or matrix multiplication: it produces output to hidden layer 2. Deconvolution matrix multiplication by transpose matrix: apply back propagation error for output to input. 3. Weight update: apply back propagation error from output to weight.,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
518,Describe: the structure on an CNN.,* Input layer * Convoluton layer (Affine transformation) * Filtering layer (Sampling) * Learning layer * Output layer,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
519,Describe: the structure on an CNN.,"CNN has basically four types of layers. They are: convolutional layer, ReLU layer, Pooling layer and the fully connected layer. We can arrange the convolutional layer and ReLU layer in different ways. One of the ways is to have 1 convolutional layer, 1 Pooling layer, 1 ReLU layer and repreat this 3 layers again and then finally a fully connected layer. Another way is to have 1 convolutional layer, 1 pooling layer again repeat the convolutional and pooling layer and then 1 ReLU layer and finally fully connected layer. Convolutional layer is used to find the feature space.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
520,Describe: the structure on an CNN.,THe CNN will have a input layer convolutional layer - Here the convolution and sub sampling of the feature maps take place Feed Forward - Neural Network layer Output layer,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
521,Describe: the structure on an CNN.,"A convolutional neural network uses the steps of convolution and subsampling alternating in the beginning. Using different kernels during convolution, many feature maps are created. The subsampling step merges the maps to reduce their amount. After some of these steps, a classical feed forward network is in the end to transform the different feature maps to one output layer.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
522,Describe: the structure on an CNN.,"A Concolutional neural network has alternating layers of convolution and pooling. The convolutional layer is applying a filter to the input, while the pooling layer sub-samples the input. In some networks this is replaced by strided convolution, which combines these two steps into one. The structure at the end of a CNN is equal to that of a regular feedforward network.",1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
523,Describe: the structure on an CNN.,,0,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
524,Describe: the structure on an CNN.,"A basic CNN can be structured into the three layers convolution, detector and pooling. In the first layer the convolution operation is performed on the inputs. In the second layer the the activation function, mostly ReLU, is applied to the result of the convolution. The last layer can be used to reduce the size of the resulting convoluted images, e.g. by max pooling.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
525,Describe: the structure on an CNN.,Convolutional Neural Network it has often images or video sequences as input. the input is computed by convolution (with different kernels) and downsampling in many steps to smaller but many more input matrices. In last step the matrices are connected to a classical FF NN.,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
526,Describe: the structure on an CNN.,A CNN conists of one or more convolution layers as well as subsampling or pooling layers followed by a fully connected standard FFN. In the convolutution layer kernels are used to create feature maps. A kernel is smaller matrix that is apllied to all possible positions on the input matrix. In the pooling stage the dimension of the rfeature map is reduced. for example by max pooling.,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
527,Describe: the structure on an CNN.,CNN uses convolutional layers to extract primitive information from pattern. First data is convolved with the first layer to extract some features. Output of this layer is passed through RELU function to rectify it. Then is downsampled by pulling layer. It basicaly chooses only relevant outputs of convolution layer for further processing. RELU is chosen instead of sigmoid because it doesnt allow gradient to vanish in backpropogation.,2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
528,Describe: the structure on an CNN.,CNN is has multiple layers and they dont use multiplication matrix.,0,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
529,Describe: the structure on an CNN.,Convolutional Neural Network(CNN) has three main layers in them 1. Convolutional Layer 2. Pooling or Subsampling Layer 3. Output layer.,1,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
530,Describe: the structure on an CNN.,"CNN has three components, * Input * Convolution stage * Feed forward network In CNN the input pass through one or more convolution stage befor it is feed into a feed forward network. The convolution stage uses a hierarchical set of filters, RELU and polling to extract low level as well as high level concepts from the input. The feed forward network along uses the output of the convolution stage and back propagation is used to make adjustment to the network weights as well the filters in the convolution stage.",2,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
531,Describe: the structure on an CNN.,,0,"Convolutional Neural Network consists of many layers such as a convolutional layer that has kernels which convolve over the input image, an activation layer (ReLU activation), pooling layer (max or average pooling), and one or more fully connected layers followed by softmax layer."
532,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",Three items to learn in a RBFN: 1. Centroids of the input clusters. 2. Widths of the clusters. 3. Weights of the synapses connecting the hidden layer and the output layer. The centroids and widths are learned in an unsupervised fashion while the weights in a supervised fashion. So an RBFN combines unsupervised and supervised learning while a regular NN is completely supervised or completely unsupervised. Learning is fast and is not so sensitive to the unsupervised part.,2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
533,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","In RBF network, we need to learn **centre** and **width** of gaussian function. We also learn **output weights** Difference between RBF and NNs: i. In RBF, there is only one hidden layer, while in NNs, there can be more than one hidden layer ii. In RBF, activation function of hidden layer is Gaussian so parameters are in euclidean norm. While, in NNs, parameters for activation function are product of weights and inputs. iii. Parameter computation is different in RBF as compute to other NNs. Like, we compute centre of cluster in RBF with the help of K-means clustering.",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
534,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",RBF network need to learn center of activation function. Differenec to other NN is that there are as many activation functions as data points. One con of Radial Basis Funtion is that due to many activation function RBF networks have a huge computational effort.,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
535,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","In RBF, we learn the centers of the radial basis functions using unsupervised clustering methods, the weights of the last output layer, and the width of our radial basis functions. As opposed to Multi layer NN, we dont need expensive backpropagation as we only need to train the last layer, while the unsupervised training algorithm does the work the RBF centers. A possbile Con would be that if the RBF centers dont represent the training data point distribution well, some data points may be hard to model.",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
536,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",1. weights 2. centres (or means) of clusters 3. $\sigma$ which is the width of the clusters Difference: Uses functions which are radially invariant. Pros: - Easy to learn - Non-linearity - Only dependent on the radial distance Cons: - Data required is more - OVerfitting,2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
537,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","An RBF network relies on a clustering algorithm. This can be e.g. k-means clustering. The three items to be learned: 1. Cluster center 2. Cluster size 3. weights connecting the hidden nodes to the output layer Difference to other NNs: - only three layers: input, hidden and output - each node in the hidden layer uses a different activation function depended on the cluster assigned to it - only output weights are trained",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
538,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",If a RBF network used a gauss function as the activation fnction these thinks have to be learned: - centroide $ci$ (unsupervised) - sigma (unsupervised) - weights of the output layer (supervised) The RBF network is easy learning and not so sesitive to the unsupervised learning part.,2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
539,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","This question is really unspecific: Difference to other NNs... - Centers - Widths - Weights The main diffrence is that the RBF uses localized activation functions and it has only one hidden layer. It applys a non-linear transformation from the input space to the hidden space and a linear transformation from the hidden space into output space. It is important to use regularization for RBF RBF work well for interpolation, so it should work good for regression",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
540,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","A Radial Basis Function Network has the following structure: - An input layer - A hidden layer, where a non-linear dimensional transformation will be used. - Each neuron of the hidden layer will have a defined center (extracted in previous steps). - A linear transformation will be used to the hidden data space, and the output will be calculated. So, the three items that must be learning in the RBF networks are: - The centers of each hidden neuron (using for example k-means neighbours algorithm). - The radial function that will be used for the non-linear transformation. - The weights applied into the output layer.",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
541,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",The three items that must be learned in RBFs are: - The center of the kernel - The size(standard deviation) of the kernel,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
542,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
543,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","- Use distance to center as argument for computation of local fields. - Use radial basis functions as activations - RFBs are only global approximators, - splitted learning instead of global learning",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
544,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",+ Kernels + Only neighbourhoods are computed based on distances. + Radius of neighbourhoods Pros + RBF are simple and easy to compute. Cons + They remember the data points,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
545,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",Differences are: * RBFN has a single hidden layer. Nonlinear hidden layer. * Linear output layer. * Argument of hidden units: Euclidean norm. * Universal approximation property. Local approximators. * Splitted Learning.,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
546,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","The mean of the k clusters, the",0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
547,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","The three items that needs to be learnt are the centers, widths and depth. Compared to other NN they have a standard 3 layer structure. They can have just one hidden layer.",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
548,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",In RBF first inputs are transformed to higer dimension using non linear transformation. This is based on unsupervised learning. Inputs are then learned using least square estimation which is an supervised learning. RBF is based on Covers theorem which states that there is higher probability that data will be linearly separable in higher dimension.,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
549,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","HERE: - RBFs are only dependent on the radial distance i.e., distance from the center to the input",0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
550,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","The three parametrs to be learnedin Generalized RBF are 1) cluster centers of the basis functions 2) spread or the width of the basis functions $\sigma$ , and 3) weights of connecting the input and the hidden layers. RBF are differenent from NNs in different ways. 1) The kernels are localized functions where as NNs are gobablized 2) They use euclidean distance in their activation functions where as NNs use inner products 3) They have a single hidden layer and output is a linear combinaation but NNs compulsarily are not the same.",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
551,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
552,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",The three open parameters of an RBF network are: 1. The centers $ci$ 2. The widths $\sigmai$ and 3. The weights $wi$ The number of centers $k$ has to be determined by trial and error.,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
553,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",In rbf the main advantage is that it follows cover's theorem and the complex pattern classification problem can be solved .,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
554,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",1. non-linear transformation function from input space to feature space 2. centers of input data that is used for each hidden neuron 3. synaptic weights connecting hidden layer and output layer,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
555,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",-,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
556,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","Three items to be learned, * origin * center Pros: * It can transform data from n dimension to infinity dimension. * It can solve non linear problems easily. Cons: * It may overfit. * Learning is slow.",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
557,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","Center of the hidden neurons, synaptic weights connecting the neurons and RBFs have only 1 hidden layer. There is a non0linear tranformation between the inputs and the hidden space and a linear tranformation between the hidden space and the output space. Pros: It can be used for non-linearly separable data.",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
558,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",1. Weigths in the network 2. the center of the clusters 3. variation of the cluster ($\sigma$) Difference: RBF always have only three layers RBF can also trained in an unspervised method RBF can also approximate any continuous function,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
559,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",- The centroids of the radial basis functions - The weights of the neurons - The amount of needed neurons A difference to other neural networks is that the centroids of the radial basis functions need to be there.,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
560,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","The centers of the clusters, the widhts of the clusters and the weights. In contrast to other NNs the output only depends on the radial distance to the center of the clusters.",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
561,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","The weights, the interpolation matrix have to be learned. The RBF maps the input space into a higher dimensional feature space nonlinearly. The feature space is mapped into the output space linearly. The output space is much smaller than the feature space. Pros: local learning Cons: feature space can be really large, curse of dimensionality",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
562,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","The clusters, the width of the basis function and the weights. The clusters and the width are learned in an unsupervised fashion. While the weights are learning by a standard supervised steepest descent method. Pros RBFs can be very easily trained. RBFs can achieve better results with less complexity. Cons Not as easy to understand",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
563,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",Centers of the radial basis functions best model (rbf) distance of each input pair pros: non-linear functions application ease to compute using covers theorem cons: high-dimensional,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
564,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","Centroids, width, and parameter of function The learning of an rbfn is splitted in an unsupervised and a supervised part. Only one layer, no vanishing gradient. Pros: easy learning the unsupervised part is not very sensitive. Cons: Difficult to approximate constants",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
565,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","1. Input layer connecting RBF to environment. 2. Hidden layer: nonolinear tranformation of input space to hidden space 3. Output layer: linear tranformation of hidden space to output space. It is different than other NNs because for learning patterns, it nonlinearly tranforms the input space to higher dimmensional space. Other NNs do not tranform input. As it tranforms input patterns to high dimmensional nonlinear space, patterns which are not separable in lower dimmensions have greater chance to be separated. But if we select basis functions equal to datapoints, problem is ill-formulated. Processing is computationallly heavy. Regualation becomes problem specific. Hence, unsupervied learning is employed to clusters data initially.",1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
566,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","data varaince Features RBF uses suport vector machine which is classifier. it uses different kernels , it doesnot have feedback cycle. it also classifies non linear classification problem. it mainly works with 2 classes C1 ,C2. other NN is can also reggression and there can be feedback (RNN)",0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
567,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",The difference of RBF to other NNS are 1. RBF has only one hidden layer wheras their is no hard limitation on number of hidden layers on other NNs 2. The activation function used in RBF is non linear.,1,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
568,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?","A RBF network learns, * The radial function * weight of the hidden to output neuron * Centroid of a cluster Difference: * A RBF is composed of input layer, 1 hidden layer and the output layer. Other NN can generally use as many hidden layers as required. * The transformation from input to hidden layer in RBF is non linear and hidden to output is linear. In most other NN both are non linear. Pros/Cons: * This is a very simple learner * There are many variations of RBF available.",2,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
569,"What are three items to be learned for an RBF network? Difference to other NNs, Pros/cons?",,0,"Three items to be learned are centers, weights, and biases. RBFN consists of a single hidden layer and a linear output layer. NN can have multiple hidden layers and a linear or non-linear output layer. Pros: RBFN is a universal approximator and it is easy to add more centers. Con: The bias is not unique.  "
570,"Describe how learning based on k-nearest neighbors works, use pseudo code!","K-nearest neighbors: 1. Take the input data to be classified. 2. Find the first nearest neighbour in terms of euclidean distance. 3. Push the class of this nearest neighbour into a list of labels. 4. Repeat step 2 and 3 for each K which needs to be odd. 5. After all K nearest labels are collected in the list, count the labels in each class. 6. Assign to the input data, the class which as maximum count (majority vote).",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
571,"Describe how learning based on k-nearest neighbors works, use pseudo code!","i. First we initialize the random points, those points are considered as centroids of clusters ii. Then, for each new points, we compute euclidean distance, and points closest to centrodis are assigned their respective clusters iii. We again re-calculate the centroids of clusters iv. Repeat 2 and 3 untill convergence is achieved, by making sure, no centroids are moving and cost function is minmized",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
572,"Describe how learning based on k-nearest neighbors works, use pseudo code!",k-nearest neighbor wants to determine encoder $\C which assigns N inputs to K clusters based on a rule to be defined.,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
573,"Describe how learning based on k-nearest neighbors works, use pseudo code!",,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
574,"Describe how learning based on k-nearest neighbors works, use pseudo code!","1. get the input 2. find the k- nearest neighbours by finding the distance (euclidean) from the input to all the nodes and selecting the k closest ones 3. Class of the input is the most frequent class in the k-neighbnours found (as such, k needs to be odd number)",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
575,"Describe how learning based on k-nearest neighbors works, use pseudo code!",$N$ number of clusters. Given sample data select $N$ different cluster centers by random. Assign all sample points to the closest cluster repeat until no further change: - recalucate the cluster centers - Assign all sample points to the closest cluster,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
576,"Describe how learning based on k-nearest neighbors works, use pseudo code!",given a fixed $k$ given a point to classify $new$ given an empty $class$ given a List of all points $L$ from 1 to k do find nearest point $x$ to $new$ in $L$ add class of neares point $x$ in list $class$ new list L = L without nearest neighboor $x$ class of new = most class in $class$,2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
577,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Define K centroids, random intialised assign each data point a class label while the is no change anymore for each k calculate the centroid of the datapoint beloging to that label for each datapoint determine the nearest centroid assign a new class label which belongs to the centroid.",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
578,"Describe how learning based on k-nearest neighbors works, use pseudo code!","K-nearest neighbors can be seen as an unsupervised learning method, where for a defined number of groups k, the nearest neighbors will be calculated. 1: For a given input data 2: Define value k 3: Get the k points that are closer to the given points.",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
579,"Describe how learning based on k-nearest neighbors works, use pseudo code!",1- randomly define a predefined number of cluster centers(CC) 2- calculate the distance of each datapoint from each CC 3- Each data point belongs to the cluster that has the least distance from its CC 4- Calculate a new CC by getting the average of all the points inside a cluster 5- Go to 2 and repeat this process untill we reach the termination condition,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
580,"Describe how learning based on k-nearest neighbors works, use pseudo code!",Firstly identify nearest neighbouring weights then choose k amount of neighbors and adapt their weights,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
581,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Initialize kneighbors = {}, for every neuron find the nearest neighbor and add it to kneighbors Return nearest kneighbors",1,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
582,"Describe how learning based on k-nearest neighbors works, use pseudo code!","** Pseudo Code ** 1. Initiate weights randomly 2. Assign labels to k-inputs that are map neuron is closest to. 3. append all inputs to map neurons using 2. 4. Find centroid of the cluster and move the map neuron to the centroid. 5. Do 2, and 4 until some convergence criteria is reached, e.g. maximum iterations is reached or no updates are performed or net distance is below some specified distance.",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
583,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Given: L; XTEST not element of L; k = number of neighbors that will taken into consideration; function classof() Set x'={}, L0=L, Classf={}; for j=1,...k do: L{j-1} \ x'; //exclude all the data points which have been identified as nearest neighbors already x'=find the closest neighbor of XTEST in Lj; //e.g. compute eucldea distance c = classof(x'); Classf=push(c) set c(xTEST)= most frequently value in Classf;",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
584,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Train the knn by storing the data labeled points. Present a test point. > Compute the distance between the test point and all the training data points. > Sort the distance, and choose the k datapoints with smallest distance. > Determine the class of the test point by majority vote.",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
585,"Describe how learning based on k-nearest neighbors works, use pseudo code!","L1 - Data set (x1,x2,x3,xn) L2 - Storing the dataset based on the number of neighbors. xtest - Test data set. So we basically have the k value to be an odd number, so that we can select a majority value. for i based on the number of l: x' = xtest - distance from the neighboring neuron i. L2 = smallest x' in this based on the number of K xtest = max(L2) We select the neurons from the neighborhood by calculating the euclidean distance based on weights. Then if K is 3, we have 3 neurons. So from that we select the label which is fixed maximum on the dataset given in the K-fields.",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
586,"Describe how learning based on k-nearest neighbors works, use pseudo code!",define criterial for finding k nearest neighbours <br> find k nearest neigbours of test input in training dataset <br> find the class to which most of the neghbours belong <br> assign that class to the test input <br>,2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
587,"Describe how learning based on k-nearest neighbors works, use pseudo code!","HERE: Learning based on K-nearest neighbors: - All the input-output samples from the training set are stored in the memory. - For a test input, find the k-nearest neighbors. - Assign the test vector with the class of the most of the neighbours in the neighborhood.",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
588,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Parameters: k -number of clusters, x datapoints , c classes 1) Initialize randomly k centroid of the custers 2) select a data point and compute the set of nearest neighbours of the point using euclidean distances. 3) Find the class that maximum number of neighbours belong to and assign the class to the datapoint. 4) Once the class is assigned, compute the centroid of each cluster or class, considering all the class members. 5) Iterate over all the datapoints and repeat over all points (from step 2) until no update in centroids is required.",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
589,"Describe how learning based on k-nearest neighbors works, use pseudo code!",,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
590,"Describe how learning based on k-nearest neighbors works, use pseudo code!",1. Given: Classified data $X$ 2. For a new sample $x$: Determine the $k$ nearest neighbours in X Output $y$ := majority vote of the class of nearest neighbours,2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
591,"Describe how learning based on k-nearest neighbors works, use pseudo code!","$ L = {x1,x2...xn} $ $L = L0$ $x' = {}$ for the input (x,d) : do { xtest }",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
592,"Describe how learning based on k-nearest neighbors works, use pseudo code!",1. identify k classified patterns that lie nearest to the test vector 2. assign the test vector to the class that is most frequently presented to the k nearest neighbors,2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
593,"Describe how learning based on k-nearest neighbors works, use pseudo code!",1. Define the number of cluster (K) 2. Generate random weights 3. Find the center of each k (mean) 4. Cluster the other outputs by determining the closest neighbor 5. Update the weights,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
594,"Describe how learning based on k-nearest neighbors works, use pseudo code!","* Choose a value for k * K represents the number of neighbors * Get a sample from the input space * Find the class based on the majority of votes received from the neighbors. * For example, if the value of k is 3, then let say there are 2 neighbors from class one and 1 neighbor from class two, then the new input sample belongs to class one.",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
595,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Step1: We randomly place the n neurons. Step2: For each data point whichever neuron is closer to it, the datapoint is assigned to that neuron. Step3: Once all the datapoints are assigned, the mean of the datapoints attached to each neuron is calculated and the neuron is shifted to the mean value. Step4: Step 2 and 3 are done until there is no more shift in the neurons position. In this way the neurons are adjusted.",0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
596,"Describe how learning based on k-nearest neighbors works, use pseudo code!",Step1 : Randomly select the k centers Step2 : Cluster the datapoints based on the centers Step3 : the centroid of the cluster becomes the new mean Step4 : repeat step 2 and 3 until there is no more evidential cahnge in the network,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
597,"Describe how learning based on k-nearest neighbors works, use pseudo code!","input: labeled data set, one unlabeled data point, number k find the k labeled points, which are closest to the given unlabeled point from these points, find the label which occurs most often assign this label to the unlabeled data point",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
598,"Describe how learning based on k-nearest neighbors works, use pseudo code!",1. Get the nearest neighbor of the current x' 2. Remove it from L 3. Get the class of the current x 4. Classify x' as the class that occurs the most often in the neighbors for 1 to K: Li = L/x' xnn = min(|x-x'|) c = getclassof(xnn) AmountofClasses.add(c) setclassof(x') = Max(AmountofClasses),2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
599,"Describe how learning based on k-nearest neighbors works, use pseudo code!","For the input data x the distance to every other data point is calculated using a distance measure. Take the k data points, which have the minimum distance to x. These are the k-nearest neighbours. The most frequent class from the neighbours is assigned as the class of the input data.",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
600,"Describe how learning based on k-nearest neighbors works, use pseudo code!",This learning is based on the memory introduced into the dataset. For each data point the nearest neighbours are found via a distance function. for each datapoint d neighbours = getknearestneighboursof(d) d.class = getmostrepresentedclass(neighbours),2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
601,"Describe how learning based on k-nearest neighbors works, use pseudo code!",for a given input compute distances to other input points pick k nearest neighboors look at labeling of neighboors decide labeling (classification) by highest number of neighboors in one class (german: Mehrheitsentscheid),2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
602,"Describe how learning based on k-nearest neighbors works, use pseudo code!",trainingset := training data define #clusters select #clusters datapoints as centroids randomly for datapoint in trainingset: calculate distance to centroid lable dataPoint according to closest centroid end for iterate over clusters: calculate centroid,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
603,"Describe how learning based on k-nearest neighbors works, use pseudo code!",(K-nearest neighbours is memory based learning.) take input x calculate calculate distance of x from each training point. Select K training points with minimum distce from the data. Fetch classes of selected K nearest points. Calculate number points per class in k nearest points. determine the class C having maximum points in k nearest pioints The class of the input point is C.,2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
604,"Describe how learning based on k-nearest neighbors works, use pseudo code!",K-nearest neighbors basically works as follows 1) the they define randomly the cluster points . 2) clacluate the mean of the equlidian distance between the data points. here the points from the previous step acts as centrioids. 3) check the variance of the clusters. 4) repeat 1-2-3 till you get the proper clusters.,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
605,"Describe how learning based on k-nearest neighbors works, use pseudo code!",1. Slect random number of neghbourhood initially 2. Find out the input which is nearest to the weight vector using competitive learning 3. Change only the input which wins 4. decrease the size of neighbourhood 5. Repeat,0,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
606,"Describe how learning based on k-nearest neighbors works, use pseudo code!","for x in inputpoints neighbours = findnearestkpoints(x) for n in neighbours v = getvoteof(n) updatevotescountfor(x,v) max = getmaxvotefor(x)",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
607,"Describe how learning based on k-nearest neighbors works, use pseudo code!","Let L be set of labeled data in memory, L ={x1,x2....xn}, while xprime is nearest point to the xtest point, in term of euclidean distance. Let classof be funtion that return class type if certain data point x. And let K be constant number of neighboring points consired in algorithm search. Initialize xprime = {}, L0 = L, listofclasses = {} for j= 1; j<=K; j++ do: Lj = L(j-1)/xprime xprime = nearest neighbor to Xtest form Lj data c = classof(xprime) listofclasses.append(c) end c(xtest):= most frequent class in listofclasses",2,"Add the new data to the members of colored or classified old data, construct a sphere with k nearest data points, find the class or color which has the maximum vote and assign the new data to the class which has the maximum vote in an unsupervised manner. "
608,Explain the Bias Variance Dilemma!,"In machine learning, a choice always needs to be made for the tradeoff between bias and variance. Bias determines how close the result is to the true value and variance determines the sensitivity to flutuations in the training dataset. If bias is reduced variance increases and vice versa. So an optimum tradeoff needs to be chosen which presents a dilemma.",1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
609,Explain the Bias Variance Dilemma!,"Bias Variance dilemma is used to analyse the generalization error of the algorithm. If the value of Bias is very high, then network does not learn relations between features and outputs correctly(overfitting) If the value of variance is very high, then network may model the random noise, and it does not learn intended ouputs(underfitting) We have to to tradeoff between Bias and Variance so that our model can generalize properly",1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
610,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
611,Explain the Bias Variance Dilemma!,"When training a model on a limited training data set, we must decide wether we accept a biased model which makes assumptions about the test data, but has a better performance on the train data, or a model with more variance which might model the entirety of the data better but be prone to data noise. Usually we have to decide on a trade off between the two, where we may select well balanced models based on VC dimensions or Cross validation results.",2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
612,Explain the Bias Variance Dilemma!,"Bias and variance are both undesirable to the learning. Bias defines how far the generated output differs from the true value. Variance defines how much the o/p change on changing the input dataset. However, in most cases, it is only possible to decrease one at the expesne of other. Thus, it is called Bias Variance Dilemma.",1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
613,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
614,Explain the Bias Variance Dilemma!,The bias is the error we make in the assumption by creating the learning machine (how much we we are away from the actual truth) the variance is how much the learning machine changes with different training data sets. if we have a high bias we habe a low variance and if we habe a low variance we habe a high bias,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
615,Explain the Bias Variance Dilemma!,You have to to a tradeoff between high bias or high variance. You cannot have both. High vaiance means the model is overfitting the data and therefore the variance on input can be quit hight. High bias means the model is generalization is to unspecific.,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
616,Explain the Bias Variance Dilemma!,"The Bias is defined as the grade of correctness that a learning algorithm will use. The Variance is defined as the grade of flexibility that the algorithm have given a model to learn. When having the Bias high, but the Variance low, the algorithm will not be flexible into data and will discard any data is not exactly the data that fits into the model. On the other hand, when having the variance high but the bias low, the algorithm will be very flexible into the data and will accept any error data as part of the model to learn.",2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
617,Explain the Bias Variance Dilemma!,- Bias: the bias is the differnce between the predicted value and the desired value in the generalization run - Variance: is the inadequity in the produced value in the regression and the desired value that we expect from the network,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
618,Explain the Bias Variance Dilemma!,Bias Variance dilemma is coming from the fact that you can not have both at the same time. Your network can not be equally great at outputing with extremely high accuracy extremely hight amount of variables. Therefore you need to find balance between the two that suits needs of your neural network.,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
619,Explain the Bias Variance Dilemma!,"It is refers to the problem of tryning to mantain a balance between two causes of errors in learning algorithms such that the network is able to generalize data beyong that used for training. Namely the bias error and the variance error. Having a high bias error may cause the network to miss important features in the training data, which leads to underfitting. High variance will make the network to memorize noise present in the training data rather than learning features, which lead to overfitting.",2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
620,Explain the Bias Variance Dilemma!,+ One cannot optimize simultaneously the learning algorithm both for learning maximum variance in the data and learning localization which can be termed as bias.,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
621,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
622,Explain the Bias Variance Dilemma!,The Bias Variance Dilemma tells us that the bias (the difference between the actual and desired output) and the variance (output difference between each trial) cannot be decreased at the same time. A complex model results in small variance and larger variance.,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
623,Explain the Bias Variance Dilemma!,"So in machine learning problem, minimizing the two main source of error simultenously does not allow the networks to be generalised very easy. If bias increase, variance decrease. And vice versa also holds. 1. Bias tells us how close we are to the true value! 2. Variance tells us how they vary for different data set. So this is a standard problem in NN",2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
624,Explain the Bias Variance Dilemma!,High value of bias means netowrk is unable to learn the data whereas higher variance means its difficult to learn the training data successfully.,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
625,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
626,Explain the Bias Variance Dilemma!,"Bias and variances are the estimation errors. Bias corresponds to the inability of the learning machine to appropriately approximate the function to be learnt. Hence this induces a deviation from the actual function Variance is the inadequacy of the training data to allow the a learning machine to succesfully learn the function. The dilemma is that , to completely learn the actual function( to reduce variance-related error), the training data required should consist of infinite samples. However, this resuts in slower convergence, inturn, bias error increases. Therefore trade of between both the errors need to be made.",2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
627,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
628,Explain the Bias Variance Dilemma!,"Bias is the difference between the predicted and true value. Variance is the range of several predicted values of the same datapoint. It is desirable to have low bias and low variance to ensure the predicted value is consistently close to the true value. The Bias Variance dilemma is that to achieve low bias, the variance becomes high and vice versa. Hence, there is always a tradeoff between the two.",2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
629,Explain the Bias Variance Dilemma!,Bias variance dilemma refers to the problem of minimizing the two sources of error bias errror and variance error simultaneously which creates probblem in generaliztion of the network. Bias error: It is the error that occurs while setting the parameters of the network variance error:It refers to how sensetive the network is to the fluctuations in the dataset.,2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
630,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
631,Explain the Bias Variance Dilemma!,Bias variance dilemma is a process of simultaneously decreasing two sources of error that prevents supervised learning algorithm from generalizing beyond the trained data.,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
632,Explain the Bias Variance Dilemma!,Bias is used to affine transform of $u$. It helps to shift the classifier line. $$v=u+b$$,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
633,Explain the Bias Variance Dilemma!,Bias: How close the estimate is to the true value. variance: How much does the estimate vary for different training sets. we always have either hugh variance low bias or low variance high bias.,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
634,Explain the Bias Variance Dilemma!,Bias : differnce between the estmated output and the actual output Variance: The range of output of a network for different training set. Bias and Variance can't be decreased at the same time for many networks. ONly one at a time can be decreased,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
635,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
636,Explain the Bias Variance Dilemma!,"When adapting the parameters of a network we can either have a small bias or a small variance. If we have a small bias the approximation of the network is close to the real one, but the variance between trials is very high. If we have a low variance, the bias can't be minimized and the network has a bigger error between the apüproximation and the real value.",1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
637,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
638,Explain the Bias Variance Dilemma!,"Ideally bias and variance would be 0 after learning a machine. However, bias and variance counteract eachother: when bias decreases, variance rises and respectively in the other direction. This leads to the dilemma that either one of the values has to be present.",1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
639,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
640,Explain the Bias Variance Dilemma!,Usualy only one of Bias and Variance can be minimized. In an RBFN for example few kernels with greater width leads to a high bias but a low variance. If you choose many kernels with smaller width the bias is low but the variance is high. Higher complexity models need more training data.,2,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
641,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
642,Explain the Bias Variance Dilemma!,Bias is an proides an affine transformation. and it is treated a extra inputs. which noramll taken as +1,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
643,Explain the Bias Variance Dilemma!,High bias and variance is desirable in input. Bias Variance Dilemma is the property of input data where if the bias is increased the variance decreases and vice versa. It is difficult to find a tradeoff between them.,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
644,Explain the Bias Variance Dilemma!,Bias: Bias means how much the prediction differs from the true value Variance: Variance means how much the prediction varies for different datasets The Dilemma is that both generally can not be reduced simultaneously. A learning machine can reduce one at the cost of other.,1,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
645,Explain the Bias Variance Dilemma!,,0,"Bias-variance dilemma is a principle supervised learning problem. The dilemma arises due to the variance of data and bias of model. When there is high bias, the model fits the training data perfectly but suffers from high variance, when the bias is low the variance reduces but the model doesn’t fit the data well. This dilemma makes the generalizability difficult to achieve."
